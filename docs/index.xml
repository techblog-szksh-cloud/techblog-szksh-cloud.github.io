<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>melody</title>
    <link>https://techblog.szksh.cloud/</link>
    <description>Recent content on melody</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>jp</language>
    <lastBuildDate>Thu, 18 Nov 2021 20:21:57 +0900</lastBuildDate><atom:link href="https://techblog.szksh.cloud/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>aqua の最近の update (v0.7.4 ~ v0.7.16)</title>
      <link>https://techblog.szksh.cloud/update-aqua-v0.7.16/</link>
      <pubDate>Thu, 18 Nov 2021 20:21:57 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/update-aqua-v0.7.16/</guid>
      <description>以前 aqua v0.7.3 がリリースされた際に aqua で組織・チームのツール群を管理 という記事を書きました。 あれからもうすぐ 2 ヶ月になり、最新バージョンは v0.7.16 になりました。 そこで v0.7.4 ~ v0.7.16 の間の更新と、関連 repository の更新を幾つか(全部ではない)紹介します。
基本的に Release Note に書いてある内容です。
 GitHub の Access Token が基本的に不要になりました Homebrew で install できるようになりました aqua.yaml がより簡潔に書けるようになりました aqua.yaml の packages を他のローカルのファイルから import できるようになりました aqua.yaml をディレクトリの階層的にネストできるようになりました aqua which コマンドをサポートしました github_archive, github_content type をサポートしました (advanced) バージョンによってパッケージの type が変更された場合にも対応できるようになりました Standard Registry の package の数が 139 =&amp;gt; 220 になりました。 aqua のための CircleCI Orb をリリースしました  GitHub の Access Token が基本的に不要になりました private repository から package をインストール場合は当然必要ですが、そうでなければ不要になりました。 これにより、 aqua を導入するハードルが下がりましたし、 GitHub API の Rate Limit に引っかかることが基本的になくなりました。</description>
    </item>
    
    <item>
      <title>2021-10 やったこと</title>
      <link>https://techblog.szksh.cloud/what-i-did-2021-10/</link>
      <pubDate>Tue, 26 Oct 2021 20:43:03 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/what-i-did-2021-10/</guid>
      <description>Job  AWS SSO の導入  Google アカウントで AWS へサインインできるように設定 AWS SSO の Terraform 管理 ssosync を Lambda で定期実行 開発者向けの移行ガイドの作成し、実際に案内 terraform, kubectl などのツールで AWS にアクセスできるかの検証   AWS WAF の COUNT, BLOCK の log を Firehose, Lambda で抽出 akoi を aqua にリプレース  Blog  2021-10-13: HashiTalks Japan 2021 で弊プロダクトの Terraform Platform について登壇しました  OSS https://github.com/pulls?q=is%3Aclosed+is%3Apublic+is%3Apr+author%3Asuzuki-shunsuke+archived%3Afalse+created%3A2021-10-01..2021-10-31+
 Profile などの更新: https://github.com/suzuki-shunsuke/suzuki-shunsuke GitHub Actions の開発 自作ツールを Homebrew でインストールできるようにした aqua: v0.7.3 =&amp;gt; v0.7.10  aqua-registry: v0.</description>
    </item>
    
    <item>
      <title>2021-09 やったこと</title>
      <link>https://techblog.szksh.cloud/what-i-did-2021-09/</link>
      <pubDate>Sat, 02 Oct 2021 21:46:15 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/what-i-did-2021-09/</guid>
      <description>仕事 今月は有休消化やシルバーウィークもあり、稼働が少なく、あまり仕事が進まなかったです。
 AWS SSO や Organizations を導入するためのロードマップの策定 AWS SSO の検証  登壇  2021-09-30 HashiTalks Japan 2021 (youtube)  Terraform Platform in Quipper (youtube) Talk (30 min)    新たに作った OSS  aqua-renovate-config  Renovate Configuration to update packages and registries of aqua    Blog  English  Tips about Renovate 2021-09-08 aqua - Declarative CLI Version Manager   https://techblog.szksh.cloud/archives/2021/09/  2021-09-25 aqua で組織・チームのツール群を管理 2021-09-05 aqua の設定ファイルをインタラクティブに生成する generate コマンド 2021-09-04 aqua v0.</description>
    </item>
    
    <item>
      <title>aqua で組織・チームのツール群を管理</title>
      <link>https://techblog.szksh.cloud/aqua-global-configs/</link>
      <pubDate>Sat, 25 Sep 2021 12:01:56 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/aqua-global-configs/</guid>
      <description>aqua v0.7.3 をリリースし、 複数の global configuration をサポートしました。
個人で使う分にはあまり嬉しい機能でもないかもしれませんが、 会社・組織・チームといった集団(以下組織で統一)で設定を共有するには便利な機能だと思います。
これまで aqua では 2 つの設定ファイルをサポートしていました。
 -c で指定した場合はそのファイル、そうでなければカレントディレクトリから探索して最初に見つかったファイル  リポジトリ直下にそのリポジトリ用の aqua.yaml をおく   global configuration (デフォルトは ~/.aqua/global/[.]aqua.y[a]ml)  個人の dotfiles とかで aqua.yaml を管理しておく    こうすることで特定のリポジトリ用の設定と個人の設定を管理することができます。
しかし、第三の設定を参照することはできませんでした。 例えばある組織で使うツールセットを aqua で管理しようと思ってもこれまでは難しかったです。
そこで AQUA_GLOBAL_CONFIG という環境変数に : 区切りで設定ファイルへのパスを設定することで先頭から順に設定ファイルを参照するようにしました。
設定ファイルの優先順位は高い方から順に次のようになります。
 -c で指定した場合はそのファイル、そうでなければカレントディレクトリから探索して最初に見つかったファイル AQUA_GLOBAL_CONFIG global configuration (デフォルトは ~/.aqua/global/[.]aqua.y[a]ml)  イメージとしては
 プロジェクト(リポジトリ)の設定 組織の設定 個人の設定  という感じです。
例えば GitHub Organizations に aqua-config というリポジトリを作成し、以下のようなファイルを用意したとしましょう。
 all.</description>
    </item>
    
    <item>
      <title>aqua の設定ファイルをインタラクティブに生成する generate コマンド</title>
      <link>https://techblog.szksh.cloud/aqua-generate/</link>
      <pubDate>Sun, 05 Sep 2021 10:17:39 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/aqua-generate/</guid>
      <description>aqua - CLI ツールのバージョン管理 aqua v0.1.0 から v0.5.0 での変更点  aqua v0.5.1 で追加された generate というサブコマンドを紹介します。
aqua では Registry を活用することで設定を記述する手間を省くことができますが、 Registry を活用するには、インストールしたいツールが Registry で定義されているか、されているとしたら name はなにか調べる必要があります。 Registry で定義されているのに見逃してしまうこともあるでしょう。
また、 aqua でツールをインストールするには version を指定する必要がありますが、多くの場合はとりあえず最新バージョンはなにかを調べることになるでしょう。
これらの手間を減らすために generate というインタラクティブなコマンドを追加しました。 これは aqua.yaml で指定されている Registry で定義されている packages の一覧から package を fuzzy search によって選択し、 更に github_release package の場合は release version の一覧を fuzzy search によって選択することで package の YAML 定義を出力するコマンドです。
使ってみるのが早いでしょう。 aqua.yaml に Standard Registry を追加した上で aqua g を実行してみます。</description>
    </item>
    
    <item>
      <title>aqua v0.1.0 から v0.5.0 での変更点</title>
      <link>https://techblog.szksh.cloud/aqua-v0.5/</link>
      <pubDate>Sat, 04 Sep 2021 11:58:42 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/aqua-v0.5/</guid>
      <description>先日 aqua v0.1.0 をリリースした記事を書いたばかりですが、 そこから更に開発を続けて v0.5.0 をリリースしたので、変更点を紹介します。
基本的に Release Note に書いてあるとおりです。
 PATH を project (aqua.yaml) 毎に設定する必要がなくなりました  ~/.aqua/bin を PATH に追加すればよくなりました direnv などを使って環境変数を追加する必要がなくなりました   install コマンドに --test option を追加し、 file.src の設定が正しいかテストできるようになりました  CI で aqua の設定をテストするのに便利   GitHub Release だけでなく、任意の URL から tool のダウンロード・インストールができるようになりました  Go や helm, Hashicorp の product のような公式サイトからダウンロードするタイプのツールも install できるようになりました   Breaking Change: inline_registry の設定の形式を変更しました aqua の設定の再利用性を高める Registry という仕組みを導入しました  standard Registry を公開しました https://github.</description>
    </item>
    
    <item>
      <title>2021-08 やったこと</title>
      <link>https://techblog.szksh.cloud/what-i-did-2021-08/</link>
      <pubDate>Thu, 02 Sep 2021 14:33:16 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/what-i-did-2021-08/</guid>
      <description>仕事  AWS IAM User を削除する際に force_destroy が true になっているか Conftest でテスト Terraform の State 分割 Terraform Modules を別リポジトリで管理して versioning git-secrets を secretlint に移行  git-secrets がメンテされてなくて、既知バグが放置されているから   CI で terraform fmt によるフォーマットの自動化 WIP: AWS WAF の COUNT, BLOCK ログを Firehose で抽出 WIP: AWS CodeBuild で Provisioning Error が発生したら自動で Retry WIP: AWS CodeBuild のための GitHub App の開発 WIP: AWS SSO について調査  OSS Contribution Renovate の GitHub Actions のドキュメントの修正をしました。 ドキュメント中に書かれたバージョンを Renovate で自動 update するようにしました。</description>
    </item>
    
    <item>
      <title>aqua - CLI ツールのバージョン管理</title>
      <link>https://techblog.szksh.cloud/aqua/</link>
      <pubDate>Sat, 28 Aug 2021 09:07:38 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/aqua/</guid>
      <description>2021-09-04 追記: aqua v0.1.0 から v0.5.0 での変更点
aqua という OSS を開発しているので紹介します。
記事の内容は aqua v0.1.0 に基づきます。将来的に仕様が変わる可能性があります。
aqua とは aqua は CLI ツールのバージョン管理のための CLI です。 aqua で管理する主な対象は GitHub Release で公開されているツールです。 YAML の設定ファイルを書いてコマンドを実行すると指定したツールをインストールすることができます。
例えば以下のような設定ファイルを書き、 aqua install というコマンドを実行すると jq, conftest などが GitHub Release からダウンロードされ、インストールされます。
packages: - name: jq registry: inline version: jq-1.6 - name: conftest registry: inline version: v0.27.0 inline_registry: - name: jq type: github_release repo_owner: stedolan repo_name: jq asset: &amp;#39;jq-{{if eq .OS &amp;#34;darwin&amp;#34;}}osx-amd64{{else}}{{if eq .</description>
    </item>
    
    <item>
      <title>AWS CodeBuild を実行する Github App を作る</title>
      <link>https://techblog.szksh.cloud/github-app-for-codebuild/</link>
      <pubDate>Mon, 16 Aug 2021 09:10:51 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/github-app-for-codebuild/</guid>
      <description>GitHub Repository の CI に CodeBuild を使う場合、 CodeBuild の Webhook integration (以下 CodeBuild GitHub integration と呼ぶことにします) を使うのが一番自然でしょう。 基本的なユースケースならこれでよいのですが、 GitHub App を活用することでより高度な CI を実現することができます。
解決したい課題  Batch Build の課題  起動・終了が遅い 全 build が成功した Batch Build を Retry できない Web UI がわかりにくい  余計な build が起動する build 単体を Retry できない   build ごとに条件設定とかできない buildspec を動的に生成できない   CodeBuild GitHub integration の課題  Build Project ごとに Repository Webhook が 1 つ作られる  webhook 1 repository あたり 20 個までしか作れない (これを裏付ける客観的なソースはないですが) webhook の数が増えると build の動作が不安定になるのを観測しています   Filter の条件が限られている(例えば PR label で filter とかできない) 複数の build を実行できない(Batch Build も 1 つとみなした場合の話)   CodeBuild の課題  Retry した場合 webhook で起動したときの環境変数が設定されない    GitHub App Amazon API Gateway と Lambda を使って GitHub App を構築します。 Lambda で webhook を受け取り、 AWS SDK を使って build を実行します。</description>
    </item>
    
    <item>
      <title>2021-07 やったこと</title>
      <link>https://techblog.szksh.cloud/what-i-did-2021-07/</link>
      <pubDate>Wed, 28 Jul 2021 06:58:45 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/what-i-did-2021-07/</guid>
      <description>今まで仕事に限定して書いてきましたが、 OSS 活動なんかにも触れてもいいんじゃないかと思ったので分かる範囲で書きます。
仕事  Docker Image を Docker Hub から ECR へ移行 Terraform  .terraform.lock.hcl を CI の中で自動で更新(commit, push)できるようにした  Terraform に詳しくない人も使うので、自動化したほうが良いと判断   tfmigrate を CI に導入 (in progress) Terraform Modules を Terraform の Monorepo とは別リポジトリで管理して versioning するようにした Route53 の管理を Roadworker から Terraform へ移行 tfmigrate を使ったリファクタリング    Event  Open Policy Agent Rego Knowledge Sharing Meetup で登壇  https://gist.github.com/suzuki-shunsuke/9372337aa62a6f8394bb136582ec068e    OSS Contribution AWS AppConfig を Terraform で管理できるようにする PR が無事マージされました。</description>
    </item>
    
    <item>
      <title>仕事でやったこと 2021-06-01 ~ 2021-06-30</title>
      <link>https://techblog.szksh.cloud/job-2021-06-01-06-30/</link>
      <pubDate>Tue, 13 Jul 2021 05:47:11 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2021-06-01-06-30/</guid>
      <description> GCP を Terraform で管理するための developer support miam を Terraform に移行 Docker Hub から ECR への移行  </description>
    </item>
    
    <item>
      <title>OPA で Table Driven Tests っぽいことをしてみる</title>
      <link>https://techblog.szksh.cloud/opa-table-driven-test/</link>
      <pubDate>Fri, 09 Jul 2021 19:38:55 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/opa-table-driven-test/</guid>
      <description>OPA で Table Driven Tests っぽく Policy を Test する方法について考えたので紹介します。
背景 先日 Open Policy Agent Rego Knowledge Sharing Meetup で発表する機会を頂きました。 発表の資料はこちら。 普段他社の事例を聞いたり OPA について話たりする機会がないので、非常に貴重な時間になりました。
その中で deeeet さんが Table Driven Tests っぽくテストしたいというようなことをおっしゃっていました。
だいたいこの辺: https://youtu.be/0YpJhrz6L0A?t=2990
その話を受けて改めて自分で考えてみたところ、できなくはないんじゃないかなという気がしたのでちょっとやってみることにしました。
サンプル せっかくなので簡単なサンプルを GitHub に用意しました。
https://github.com/suzuki-shunsuke/example-opa-table-driven-tests
今回は aws_cloud_watch_log_group の retention_in_days が設定されていることをチェックする Rule の Test をします。
 Rule: https://github.com/suzuki-shunsuke/example-opa-table-driven-tests/blob/main/policy/cloudwatch_log_retention_in_days.rego Policy Test: https://github.com/suzuki-shunsuke/example-opa-table-driven-tests/blob/main/policy/cloudwatch_log_retention_in_days_test.rego  テストケースを seeds という list で定義し、どれか一つでも false だったら fail するようにしています。 テストケースの中身は
 msg: テストケースを示すメッセージ。テストが失敗したときの trace に含める resource: rule の input exp: rule の評価結果の期待値  になっています。</description>
    </item>
    
    <item>
      <title>Terraform で空の AWS Lambda Function を作る方法</title>
      <link>https://techblog.szksh.cloud/create-empty-lambda-by-terraform/</link>
      <pubDate>Thu, 24 Jun 2021 21:25:24 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/create-empty-lambda-by-terraform/</guid>
      <description>Terraform で空の AWS Lambda Function を作ろうとした際にちょっとハマったのでやり方を書いておきます。
「空の Lambda Function」という表現は適切ではないかもしれませんが、 Lambda で実行するコードのデプロイは Terraform 以外のツールでやるけど、 Lambda Function の作成は Terraform で行うので、 dummy のコードを指定して Terraform で Lambda を作るという話です。
自分は今は lambroll というツールで Lambda をデプロイしています。 lambroll は Lambda Function も作ってくれるので Terraform で作る必要は必ずしもありません。
しかし Lambda Function に関連するリソースを Terraform で管理する場合、 Lambda Function も Terraform で作ると Lambda Function の ARN や Invoke ARN を参照できます。
また lambroll でデプロイする場合も先に Terraform で IAM Role を作成する必要がありますが、 Terraform で aws_lambda_permission のようなリソースを作成するには Lambda Function が先に作られている必要があるので、 互いに依存関係が発生し、面倒なことになります。</description>
    </item>
    
    <item>
      <title>仕事でやったこと 2021-05-01 ~ 2021-05-31</title>
      <link>https://techblog.szksh.cloud/job-2021-05-01-05-31/</link>
      <pubDate>Sun, 30 May 2021 08:19:32 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2021-05-01-05-31/</guid>
      <description> SRE チームの新メンバーのオンボーディングのサポート GCP  dev からのリクエストに応じて権限付与したり対応 Terraform による GCP の管理 CI/CD の整備 Workload Identity Federation について調べた Terraform による IAM 管理の仕方を検討   Conftest  opa fmt によるフォーマット(CI も導入) Policy Testing (CI も導入)   Upgrade Terraform to v0.15.4 miam の Terraform 移行を検証  </description>
    </item>
    
    <item>
      <title>仕事でやったこと 2021-04-01 ~ 2021-04-30</title>
      <link>https://techblog.szksh.cloud/job-2021-04-01-04-30/</link>
      <pubDate>Wed, 28 Apr 2021 09:28:01 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2021-04-01-04-30/</guid>
      <description> SRE チームの新メンバーのオンボーディングのサポート Lambda の Monorepo  幾つか実際に Function 作った(developer support) 幾つかの Release Strategy の実装・検証  シンプルな GitHub Flow Git Flow をアレンジしたリリースフロー Canary Release WIP: AWS AppConfig を用いた Dark Launch     IAM User の初期パスワード送信の自動化 Terraform  Renovate による Terraform の patch update の自動化 Docker を使ったローカル開発環境の改善 ローカルで terraform init したら .terraform.lock.hcl が更新される問題の対応 https://techblog.szksh.cloud/terraform-providers-lock/   GCP の Terraform 管理  調査 WIP   Conftest  社内の Rego の活用事例をまとめた opa fmt によるフォーマット(CI も導入) Policy Testing (CI も導入)    </description>
    </item>
    
    <item>
      <title>PR で変更されたファイルや PR Label に応じて matrix build を実行する Github Actions の Workflow のサンプルを書いてみた</title>
      <link>https://techblog.szksh.cloud/example-github-actions-dynamic-matrix/</link>
      <pubDate>Sun, 25 Apr 2021 16:09:47 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/example-github-actions-dynamic-matrix/</guid>
      <description>GitHub Actions の勉強がてら Pull Request (以下 PR) で変更されたファイルや PR Label に応じて Matrix build を実行する Github Actions の Workflow のサンプルを書いてみました。
https://github.com/suzuki-shunsuke/example-github-actions-dynamic-matrix
Monorepo で同じ Job を PR で変更されたものに対してだけ実行したい、 けど workflow をサービスごとに定義するのはめんどいみたいな場合に使えるかもしれません。
勉強がてらちょっと書いてみて軽く動作確認しただけなので、バグってる、あるいは実用的ではないかもしれません。
ここでは Monorepo の CI を GitHub Actions で実行する場合を考えます。
GitHub Actions では path filter を用いて workflow の実行有無を制御することができます。 そこでサービスごとに workflow を作成し、 path filter を設定することでそのサービスが更新されたときのみそのサービスの CI を実行するということが簡単にできます。
しかし多くのサービスが含まれる Monorepo で各サービスに同じ Job を実行したい場合を考えてみましょう。 その場合サービスを追加するたびに workflow を追加していく必要があります。 まぁ .github/workflows 配下に 1 つ YAML をコピペで作成するだけといえばそれまでなのですが、それすらも省略したいとしましょう。
Terraform の CI/CD を CodeBuild に移行した話では CodeBuild の Batch Build の buildspec を PR で変更されたファイルおよび PR Label に応じて動的に生成しています。 これの良いところは、サービスを追加したり、リネームしたり、削除したりしても CI をイジる必要がまったくないところです。</description>
    </item>
    
    <item>
      <title>terraform init で lock ファイルが更新される問題の対応</title>
      <link>https://techblog.szksh.cloud/terraform-providers-lock/</link>
      <pubDate>Sat, 24 Apr 2021 21:57:27 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/terraform-providers-lock/</guid>
      <description>Terraform v0.14 で local で terraform init すると lock ファイルが更新されてしまう問題に対応しました。
結論を最初に言うと、 100 以上の Terraform 環境をいい感じに v0.14 に upgrade した方法で紹介している方法で Renovate で Terraform Provider を update する際に terraform init -upgrade を実行して lock ファイルを更新してコミット・プッシュしているのですが、 その際に terraform providers lock -platform=darwin_amd64 を実行するようにしました。
Terraform v0.14 で lock ファイル .terraform.lock.hcl が導入されました。 Renovate で Terraform Provider を update する際にも lock ファイルを更新する必要があるので、 terraform init -upgrade を実行して lock ファイルを更新してコミット・プッシュしています。 なのですが、ローカルで terraform init を実行するとなんか lock ファイルが更新されることが良くありました。しばらく放置していたのですが、 developer から「なんかファイル更新されたんだけど、これコミットしていいの？」と聞かれ、このまま放っておいて困惑させたりもやっとさせたりするのは良くないなと思い、調べてみました。
lock ファイルについて .</description>
    </item>
    
    <item>
      <title>Terraform Module の Template という使い方</title>
      <link>https://techblog.szksh.cloud/terraform-module-template/</link>
      <pubDate>Sat, 03 Apr 2021 11:04:25 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/terraform-module-template/</guid>
      <description>Terraform Module の使い方として Terraform Module のテンプレートをコピペして使うというアプローチを紹介します。
Terraform の設定ファイル(以下 tfファイル) を書く際、毎回一から書くのは大変です。 多くの場合、既存のコードを再利用したほうが楽でしょう。
Terraform のコードの再利用の仕組みとして、 Module があります。 Module は勿論便利なのですが、使い方には注意が必要で、「安易に Module 化するな。使うな」というふうな考え方もあるでしょう。 自分も基本的に同意見で、 Module を共用するようになると Module への変更がしづらくなったり、パラメータがどんどん増えて複雑になったりします。
例えば次のように共用の local Module を作成するアプローチがあります。
modules/ lambda-base/ README.md main.tf variables.tf outputs.tf services/ foo/ staging/ main.tf # リポジトリ直下の modules/lambda-base を参照 production/ main.tf # リポジトリ直下の modules/lambda-base を参照 こうすると modules 配下の Module を変更した際にその Module を使っているすべてのサービスに影響が出てしまい、 サービスのオーナーが様々だったり、曖昧だったり不在だったりすると変更が難しいですし、どんどん Module が複雑になったりします。
Module を別のリポジトリでバージョニングして管理し、バージョンを指定するようにするというやり方もありますが、 結構複雑というか考えることが多いアプローチだとは思います。
Terraform にそこまで詳しくない developer にも書いてもらうとなると、シンプルなアプローチにするのが望ましいでしょう(当然これは組織によりますが)。
そこで Module のテンプレートを用意し、 Module を使いたくなったらそれをコピペして使うというアプローチがあります。 例えば lambda-base という Module の Template を foo というサービスの staging 環境と production 環境で使う場合、次のような感じになります。</description>
    </item>
    
    <item>
      <title>仕事でやったこと 2021-03-01 ~ 2021-03-31</title>
      <link>https://techblog.szksh.cloud/job-2021-03-01-03-31/</link>
      <pubDate>Sun, 28 Mar 2021 23:36:55 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2021-03-01-03-31/</guid>
      <description> ブログの執筆  100 以上の Terraform 環境をいい感じに v0.14 に upgrade した方法 Pull Request の terraform plan の実行結果を S3 に保存して安全に apply   Terraform のドキュメント作成  CI/CD でやってること ローカルでの開発環境 setup etc   CI の改善  CircleCI の job の branch filter を設定して余計な job が実行されないようにした (進行中) デプロイをサービスごとに分割   (進行中) Lambda を爆速でデプロイするためのプラットフォームづくり  Terraform + lambroll + AWS CodeBuild   MongoDB upgrade (Atlas) On Boarding のドキュメントの整理  </description>
    </item>
    
    <item>
      <title>仕事でやったこと 2021-02-01 ~ 2021-02-28</title>
      <link>https://techblog.szksh.cloud/job-2021-02-01-02-28/</link>
      <pubDate>Sat, 27 Feb 2021 20:51:56 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2021-02-01-02-28/</guid>
      <description> Terraform リポジトリの統合 Terraform を v0.14 へ upgrade デプロイをサービスごとに分割 CircleCI の設定ファイルの分割  https://blog.studysapuri.jp/entry/2020/12/01/080000 と同様のことを別のリポジトリでもやった   Renovate の設定を改善し、安全性と運用の負担軽減、オープンなままの PR の削減 miam の Rate Limit 問題の解消  </description>
    </item>
    
    <item>
      <title>仕事でやったこと 2021-01-01 ~ 2021-01-31</title>
      <link>https://techblog.szksh.cloud/job-2021-01-01-01-31/</link>
      <pubDate>Thu, 11 Feb 2021 08:13:10 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2021-01-01-01-31/</guid>
      <description> CI: PR コメントの非表示  github-comment, tfcmt  非表示以外にも細かな改善入れている     Renovate  label を設定 Renovate の PR に便利なリンクを追加   Terraform  apply こけたときに plan 再実行して S3 に保存している plan file 及び PR コメントを更新 tfnotify を tfcmt にリプレース  tfnotify を fork した      </description>
    </item>
    
    <item>
      <title>仕事でやったこと 2020-12-01 ~ 2020-12-31</title>
      <link>https://techblog.szksh.cloud/job-2020-12-01-12-31/</link>
      <pubDate>Thu, 11 Feb 2021 08:02:11 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2020-12-01-12-31/</guid>
      <description>2020-12-01 から 2020-12-31 にかけて仕事でやったことを書ける範囲で書きます。
 AWS SAM Application の開発 Renovate の PR にリンクを追加 Terraform  Terraform の CI に関して日々行っている改善点・変更点をチームにシェア Docker Compose を用いてローカルで開発しやすいように改善 ドキュメント・コードコメントの追加 リファクタリング  不要なコードの削除 不要な secret を削除 不要な変数の削除 data.terraform_remote_state を local values に置換 なぜか環境変数でパラメータを渡していた箇所を、 local value に置換   CI に tflint の導入 対象の build が 1 つの場合 batch build を実行しないようにする  Terraform の CI/CD を CodeBuild に移行した話 の改良 Batch Build の起動に時間がかかる問題の解消   master の HEAD じゃなくても apply できるようにする plan file を S3 に保存 refactor: tfsec で設定ファイルを使うようにする Renovate の PR が多すぎて鬱陶しい問題の対応  automerge されるものは reviewer を設定しないようにした prConcurrentLimit を 1 にした branch protection Require branches to be up to date before merging を無効化     kube-linter  Rule に基づいて manifest の修正   miam でリソースが削除されそうなときに警告をするようにした ブログの執筆  Renovate の Tips Terraform の CI/CD を CodeBuild に移行した話 巨大な .</description>
    </item>
    
    <item>
      <title>terraformer で雑に生成した tf ファイル と state を分割したくてツールを書いた</title>
      <link>https://techblog.szksh.cloud/tfmigrator/</link>
      <pubDate>Sun, 31 Jan 2021 14:53:23 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/tfmigrator/</guid>
      <description>terraformer で雑に生成した Terraform の設定ファイル (以下 tf ファイル) と state を分割したくてツールを書きました。
tfmigrator
経緯 miam から Terraform へ移行したい miam というツールで管理されている大量のリソースを Terraform で管理したくなりました。 多くの AWS Resource は Terraform で管理されていますが、 IAM に関しては miam で管理されています。 なぜ Terraform ではなく miam で管理されているかというと、当時のことは自分には分かりませんが、歴史的な経緯もあると思います。 昔は今よりも Terraform の表現力が豊かではなく、 Ruby で自由にかける miam のほうが扱いやすかったとか、 miam だと miam でリソースを管理することを強制できるため、権限管理を厳格にやるという観点では都合が良いという点もあるかと思います。
ではなぜ Terraform で管理したくなったかというと、 一番大きな理由は miam で頻繁に rate limit に引っかかるようになったからです。 Terraform にしろ miam にしろ CI/CD で test, apply が実行されるようになっています。 miam では毎回全部のリソースを対象に処理が実行されるため、リソースの数が増えるにつれて rate limit に引っかかりやすくなります。 CI を rerun すれば成功するのですが、悪いときは 3 回連続で rate limit に引っかかり、 4 回目でようやく成功するということもありました。</description>
    </item>
    
    <item>
      <title>skaffold を使って GitOps する</title>
      <link>https://techblog.szksh.cloud/gitops-skaffold/</link>
      <pubDate>Mon, 11 Jan 2021 16:24:44 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/gitops-skaffold/</guid>
      <description>skaffold を用いてマニフェストを動的に生成しつつ GitOps する方法を考えたので紹介します。 なお、現時点ではあくまで考えてみただけで実際に導入したりはしていません。
GitOps はマニフェストを Git リポジトリにコミットしないといけないわけですが、 Docker image をビルド、プッシュし、マニフェストの image tag を書き換えるという一連の処理をどうやってやるのがいいのか 個人的に考えていました。
自分は FluxCD には詳しくないのですが、 FluxCD では registry をポーリングして自動で最新のタグに書き換える機能があるそうですね。
https://toolkit.fluxcd.io/guides/image-update/
ただし、まだ alpha であることと、 semver に従っていないといけないようです。 これだと master branch が update されるたびに image をビルドして sha でタグを付与するみたいな運用は難しそうです。
Skaffold だとマニフェストの image tag を自動で書き換えてくれる機能があります。 加えて skaffold render コマンドを使うと manifest の apply はせずにファイルへの出力だけやってくれます。 出力された manifest を Git リポジトリに commit, push すれば GitOps が実現できそうです。
How リポジトリを 2 つ用意します。
 app: アプリケーションのコードとマニフェストを管理するリポジトリ manifest: GitOps が連携するマニフェストを管理するリポジトリ  app は Monorepo になっているとします。ディレクトリ構成は次のような感じをイメージしています。</description>
    </item>
    
    <item>
      <title>tfnotify を fork した</title>
      <link>https://techblog.szksh.cloud/fork-tfnotify/</link>
      <pubDate>Sat, 02 Jan 2021 19:42:10 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/fork-tfnotify/</guid>
      <description>mercari/tfnotify を Fork して 2 つほど OSS を作りました。
 https://github.com/suzuki-shunsuke/tfnotify - tfnotify と互換性あり https://github.com/suzuki-shunsuke/tfcmt - tfnotify と互換性がない  開発の経緯 これまで tfnotify を便利に使わせてもらってたのですが、幾つか改善したいと思うところがあり、本家に PR を投げました。 しかし残念ながらこれまでのところ反応がなく、そこまで本家が活発ではないこと、また他にも色々改修したいところがあったことから、自分でフォークしてメンテすることにしました。 最初は互換性を維持しながら suzuki-shunsuke/tfnotify を開発していました(今もしています)。 しかし、開発を進めるに連れ、自分にとって必要のないプラットフォームなどに関するコードが邪魔であると感じ、それらを消したバージョンを別に開発することにしました。 互換性がなくなることから、名前も変えて tfcmt としました。
https://github.com/suzuki-shunsuke/tfcmt
こういった経緯から、 tfcmt のほうを優先的に開発していますが、 tfcmt で実装した機能を後から suzuki-shunsuke/tfnotify にも実装してたりもします。
Fork 元のバージョン suzuki-shunsuke/tfnotify は mercari/tfnotify v0.7.0 fb178d8 をフォークしました。 一方 tfcmt は suzuki-shunsuke/tfnotify v1.3.3 をフォークしました。
mercari/tfnotify との違い 本家との違いは Release Note とドキュメントを参照してください。
 suzuki-shunsuke/tfnotify  https://github.com/suzuki-shunsuke/tfnotify/releases https://github.com/suzuki-shunsuke/tfnotify/blob/master/COMPARED_WITH_TFNOTIFY.md   suzuki-shunsuke/tfcmt  https://github.com/suzuki-shunsuke/tfcmt/releases https://github.</description>
    </item>
    
    <item>
      <title>2019-10 から今(2020-12-31)に至るまで仕事でやっていること</title>
      <link>https://techblog.szksh.cloud/my-work-2020/</link>
      <pubDate>Thu, 31 Dec 2020 11:00:40 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/my-work-2020/</guid>
      <description>2019-10-01 から今の職場で SRE として働いています。 その中で自分がどういうことをやっているかという話をします。
2020-12-31 現在の内容です。
要約  プロダクト横断的な SRE チームで、プロダクトのプラットフォームを運用・開発している 特に CI/CD の改善が得意 developer に CI/CD をいわばサービスとして提供しており、 DX の改善に取り組んでいる Monorepo の負の側面(CIが遅い、関係ないTestがこけるetc)の解消にも取り組んでいる 自分が直面している課題を解決する OSS を色々開発している  キーワード  SRE Monorepo CI/CD Developer Experience Terraform Go / Shell script k8s CircleCI / CodeBuild Conftest Renovate  より具体的にやっていることを書いた記事  個人ブログ  https://techblog.szksh.cloud/job-2020-10-01-10-31/ https://techblog.szksh.cloud/job-2020-07-01-09-30/ https://techblog.szksh.cloud/job-2020-04-01-06-30/ https://techblog.szksh.cloud/job-2020-01-01-03-31/ https://techblog.szksh.cloud/job-2019-10-01-12-31/   会社ブログ  Renovate の Tips Terraform の CI/CD を CodeBuild に移行した話 巨大な .circleci/config.yml を分割した話 Docker Hub の Rate Limit 問題に対応した話 CI の修正をリリース前に本番と同じ条件下で検証出来る仕組みを構築した話    何をやっているか プロダクト横断的な SRE チームで、プロダクトのプラットフォームを運用・開発しています。</description>
    </item>
    
    <item>
      <title>仕事でやったこと 2020-11-01 ~ 2020-11-30</title>
      <link>https://techblog.szksh.cloud/job-2020-11-01-11-30/</link>
      <pubDate>Wed, 30 Dec 2020 05:44:14 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2020-11-01-11-30/</guid>
      <description>2020-11-01 から 2020-11-30 にかけて仕事でやったことを書ける範囲で書きます。
 github-ci-monitor を導入し、 CI の失敗を通知  https://github.com/suzuki-shunsuke/github-ci-monitor   Terraform  Upgrade AWS Provider: Renovate で自動更新する仕組みの改善 Upgrade Terraform from v0.12 to v0.13 tfsec の導入 PR の label によって CI(plan/apply) の実行対象を追加できるようにした CircleCI から CodeBuild への移行 tfmigrate の検証   Monorepo の CI の高速化 (CircleCI)  k8s manifest の test を、変更があったものに対してだけ実行するようにした   kube-linter の導入 Renovate  https://blog.studysapuri.jp/entry/2020/12/10/080000 additionalBranchPrefix によるブランチの分割 depName を使ったリファクタリング   .circleci/config.yml の分割 ブログの執筆  Docker Hub の Rate Limit 問題に対応した話    </description>
    </item>
    
    <item>
      <title>Renovate の PR に便利なリンクを追加</title>
      <link>https://techblog.szksh.cloud/renovate-add-compare/</link>
      <pubDate>Mon, 28 Dec 2020 14:32:31 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/renovate-add-compare/</guid>
      <description>Renovate による PR をレビューする際、差分がなんなのか分かりづらいときがあります。 例えば data source が github-release の場合、 PR の description に Release Note が含まれており、コードの差分も link があるので便利です。 一方 helm data source の場合、 そういったものがなく差分がなんなのか分からないことがあります。 そういう場合、 prBodyNotes を利用して link を追加すると便利です。
例えば、 datadog helm chart の場合
{ &amp;#34;datasources&amp;#34;: [&amp;#34;helm&amp;#34;], &amp;#34;packageNames&amp;#34;: [&amp;#34;datadog&amp;#34;], &amp;#34;prBodyNotes&amp;#34;: [ &amp;#34;[compare](https://github.com/DataDog/helm-charts/compare/datadog-{{currentVersion}}...datadog-{{newVersion}})&amp;#34; ] } とすると、 PR の description にリンクが追加されます。地味ですが便利です。 Release page へのリンクを追加しても便利かもしれませんね。
ex. https://github.com/suzuki-shunsuke/test-renovate-2/pull/28
template で使える変数は https://docs.renovatebot.com/templates/ を参照してください。 helm chart ごとに設定を書かないといけないのが面倒ですが、仕方ないですね。</description>
    </item>
    
    <item>
      <title>Renovate と Dependabot の比較</title>
      <link>https://techblog.szksh.cloud/compare-renovate-dependabot/</link>
      <pubDate>Sat, 05 Dec 2020 12:37:17 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/compare-renovate-dependabot/</guid>
      <description>普段 Renovate を主に使っている自分が、 Dependabot と Renovate の違いについて調べてみました。 普段 Renovate を主に使っているので、 Renovate 寄りの内容になっています。 気分を害する人がいましたら申し訳ありません。 Dependabot の理解が浅いので間違ってたら指摘してもらえると助かります。 2020-12-01 時点の情報です。
設定項目の数  https://docs.github.com/en/free-pro-team@latest/github/administering-a-repository/configuration-options-for-dependency-updates#directory https://docs.renovatebot.com/configuration-options/  まずは設定のドキュメントを見比べると、 Renovate のほうが設定項目が多いです。 Renovate はよく言うと設定項目が多く、柔軟な設定ができるといえる一方、すべての設定を理解し使いこなすのは難しいです。 決して日本語の情報も多くないので、色々試行錯誤したりすることもあります。 Dependabot の場合、設定がそんなに多くなく割と分かりやすい印象があります。
scheduling  https://docs.github.com/en/free-pro-team@latest/github/administering-a-repository/configuration-options-for-dependency-updates#scheduleinterval https://docs.renovatebot.com/configuration-options/#prhourlylimit https://docs.renovatebot.com/configuration-options/#schedule  Dependabot は schedule の設定が必須です。 Renovate でも schedule の設定は出来ます。
Dependabot は 1 個 1 個設定しないといけない？ Renovate は renovate.json さえ作れば中身がほぼ空でも勝手に update されます。 逆に対象を絞りたかったら明示的に指定する必要があります。
一方で Dependabot は対象を 1 つ 1 つ指定しないといけないようですね。
もちろん、これは必ずしも悪いことではないですし、良い面もあります。 設定が明示的に書かれていたほうが挙動を理解しやすいですしね。
ただし、数が多いと大変ですし、サービスを追加するたびに設定を追加しないといけなさそうです。
Renovate は .circleci/config.yml などの update もサポート  https://docs.</description>
    </item>
    
    <item>
      <title>Terraform の Docker Provider の Collaborator になりました</title>
      <link>https://techblog.szksh.cloud/collaborator-of-terraform-docker-provider/</link>
      <pubDate>Thu, 03 Dec 2020 09:07:32 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/collaborator-of-terraform-docker-provider/</guid>
      <description>先日 kreuzwerker/terraform-provider-docker の Collaborator になりました。 kreuzwerker/terraform-provider-docker は Terraform の Docker Provider であり、 Docker コンテナや image, network などを管理できます。 元々は Hashicorp の Official Provider であった terraform-providers/terraform-provider-docker が kreuzwerker/terraform-provider-docker に移管され、 Community Provider になりました。 元のリポジトリは hashicorp org に移され archive されています。
Collaborator になった経緯 リポジトリが移管される際に、メンテナを募集していて過去に contribution していた自分にも声をかけていただきました。
https://github.com/hashicorp/terraform-provider-docker/issues/306
Contributor になった経緯 自分がこの provider に contribution した経緯は、 Terraform の Hands on を書くのに丁度よい provider を探していたことでした。
Hands on の題材として Docker コンテナを作ったりできたらいいんじゃないかなと思って Docker provider を試してみました。 しかし当時の docker_container リソースは read をちゃんとサポートしていませんでした。 なので import や update がまともに動きませんでした。 それを見かねて修正して PR を投げたのが最初です。</description>
    </item>
    
    <item>
      <title>仕事でやったこと 2020-10-01 ~ 2020-10-31</title>
      <link>https://techblog.szksh.cloud/job-2020-10-01-10-31/</link>
      <pubDate>Sun, 29 Nov 2020 11:48:17 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2020-10-01-10-31/</guid>
      <description>2020-10-01 から 2020-10-31 にかけて仕事でやったことを書きます。 勿論全部は書けないのでいくつかピックアップして書きます。
 Terraform  CodeBuild Batch Build による dynamic な build pipeline の導入 Conftest の実行方法を修正  https://www.openpolicyagent.org/docs/latest/terraform/ tf ファイルに対して conftest を実行するのではなく、公式の方法に従ってテストするようにした   aws_cloudwatch_log_group.retention_in_days が設定されているか Conftest でテスト  コスト削減   tfnotify が parse に失敗した場合、 github-comment でコメント  https://techblog.szksh.cloud/post-tfnotify-parse-error/     Docker Hub 認証するようにした  Docker Hub の Rate Limit 問題に対応した話   kustomize build のテスト(失敗したら PR にコメントをして、なぜ失敗したか分かるようにした)  元々 kustomize build に失敗して CI がこけても、なぜこけたのか分かりにくく、 SRE に問い合わせが来て調べるみたいなことがあった どの k8s manifest の kustomize build に失敗したのか、 PR に分かりやすくコメントするようにした   CI で kubectl apply &amp;ndash;server-dry-run によるテストを導入 Monorepo  差分検知・デプロイパイプラインの改善   Renovate  automerge の導入  renovate-approve 便利   CI で renovate-config-validator による validation の導入  https://docs.</description>
    </item>
    
    <item>
      <title>仕事でやったこと 2020-07-01 ~ 2020-09-30</title>
      <link>https://techblog.szksh.cloud/job-2020-07-01-09-30/</link>
      <pubDate>Sun, 29 Nov 2020 11:36:44 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2020-07-01-09-30/</guid>
      <description>2020-07-01 から 2020-09-30 にかけて仕事でやったことを書きます。 勿論全部は書けないのでいくつかピックアップして書きます。
 Terraform  CircleCI から CodeBuild への移行  Security + DX 10月以降も継続   tfnotify の導入  Monorepo で導入するには一工夫が必要 コメントが消える挙動がドキュメント化されてなくて軽くハマった     Renovate の導入  Regex Manager 便利   Monorepo  差分検知・デプロイパイプラインの改善   DataDog からのアラートのハンドリング業務の改善  どんなアラートがあって、どう対応したかなどのナレッジの共有の改善 割とアナログな手法だが、ちゃんと work している    </description>
    </item>
    
    <item>
      <title>仕事でやったこと 2020-04-01 ~ 2020-06-30</title>
      <link>https://techblog.szksh.cloud/job-2020-04-01-06-30/</link>
      <pubDate>Sun, 29 Nov 2020 11:20:07 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2020-04-01-06-30/</guid>
      <description>2020-04-01 から 2020-06-30 にかけて仕事でやったことを書きます。 勿論全部は書けないのでいくつかピックアップして書きます。
 Monorepo  CI で kustomize build の diff を PR にコメント Conftest で k8s manifest の validation の導入 Go で書かれた差分検知のコードのリファクタリング CI の高速化・コスト削減   GitOps  GitOps を導入するためのツールの開発のサポート   Terraform  input variables を local value に置き換え   GuardDuty の導入  Terraform で IaC False Positive なアラートが多くてハンドリングできてない   ブログの執筆  CI の修正をリリース前に本番と同じ条件下で検証出来る仕組みを構築した話   Argo Workflows の検証 MongoDB Atlas への移行サポート  Go で Restore Job の開発 オペレーション    </description>
    </item>
    
    <item>
      <title>仕事でやったこと 2020-01-01 ~ 2020-03-31</title>
      <link>https://techblog.szksh.cloud/job-2020-01-01-03-31/</link>
      <pubDate>Sun, 29 Nov 2020 10:50:11 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2020-01-01-03-31/</guid>
      <description>2020-01-01 から 2020-03-31 にかけて仕事でやったことを書きます。 勿論全部は書けないのでいくつかピックアップして書きます。
 Monorepo  更新検知のシェルスクリプトを Go でリプレース  CI の修正をリリース前に本番と同じ条件下で検証出来る仕組みを構築した話   CircleCI のコードリストアの改善(コスト削減・高速化)  リポジトリが非常にでかいので全部 checkout, restore すると時間がかかる Job ごとに必要最小限のコードだけをリストアすることで高速化     サーバの OS upgrade Ansible  CI の更新検知で、 PR の label で対象を指定できるようにした(コードに変更がなくてもテストが実行できるようにした)  たまにテストしたいときはある それまではてきとうにコードを修正しないとテストが実行されなかったが、 PR の label で対象の playbook を指定できるようにした   key=value 形式を YAML に変換  https://www.ansible.com/blog/ansible-best-practices-essentials  Use Native YAML Syntax       .circleci/config.yml のリファクタリング  job を parameterize して共通化したり parameterize された command を使って共通化したり コード量の大幅な削減   Terraform  CircleCI の環境変数を設定することで、 master での CI を一時的に禁止できるようにした  State を弄ってたりするときに予期せぬ apply が実行されないようにするため   master branch の CI が終わるまで wait  master の CI が走っている間に PR の CI でまだ apply されていないリソースが差分として出るのを防ぐ   shfmt, shellcheck の導入  CI/CD でシェルスクリプトを書いているので、それらを lint   State Locking の導入 Terraform Cloud の検証  結果、見送り すでに CI/CD pipeline を構築している自分たちにとっては、わざわざ移行するメリットが薄いと判断     MongoDB upgrade Jenkins Alternative の検証  RunDeck Argo Workflows 結局、ローカルで検証した程度    </description>
    </item>
    
    <item>
      <title>仕事でやったこと 2019-10-01 ~ 2019-12-31</title>
      <link>https://techblog.szksh.cloud/job-2019-10-01-12-31/</link>
      <pubDate>Sun, 29 Nov 2020 10:38:57 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/job-2019-10-01-12-31/</guid>
      <description>2019-10-01 から 2019-12-31 にかけて仕事でやったことを書きます。 勿論全部は書けないのでいくつかピックアップして書きます。
 Terraform  terraform fmt の導入 Terraform の upgrade v0.11 =&amp;gt; v0.12 State の分割 3 =&amp;gt; 40 弱(正確な数は忘れた) Conftest による lint: Remote Backend のパスを間違えるとまずいので test を導入 新しいサービスを追加するときのための generator (シェルスクリプト)を開発 CODEOWNERS の設定 突撃隣の Terraform リリースブランチをやめて GitHub Flow に移行 古いリビジョンで apply の実行の禁止(CI がこけるようにした)   Ansible  CI の高速化 Jenkins job を CircleCI の scheduled job にリプレース(脱 Jenkins) ローカルでの開発環境の改善  コンテナを使い回せるようにする   goss に置き換えて高速化   CI の改善  高速化(無駄な処理の削減)    </description>
    </item>
    
    <item>
      <title>Splitting .circleci/config.yml</title>
      <link>https://techblog.szksh.cloud/splitting-circleci-config/</link>
      <pubDate>Sat, 07 Nov 2020 14:43:10 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/splitting-circleci-config/</guid>
      <description>In this post I introduce how to split a huge .circleci/config.yml.
CircleCI doesn&amp;rsquo;t support to split .circleci/config.yml, so we manage all workflows and jobs configuration into one file .circleci/config.yml. If the repository is Monorepo, the more the number of services increases, the more the size of .circleci/config.yml becomes large and it&amp;rsquo;s hard to maintain .circleci/config.yml. By splitting .circleci/config.yml per service, it makes easy to maintain .circleci/config.yml and we can configure split file&amp;rsquo;s CODEOWNERS.</description>
    </item>
    
    <item>
      <title>github-ci-monitor: CI のステータスを DataDog で監視</title>
      <link>https://techblog.szksh.cloud/github-ci-monitor/</link>
      <pubDate>Sun, 01 Nov 2020 17:51:44 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/github-ci-monitor/</guid>
      <description>自作の OSS github-ci-monitor の紹介です。
GitHub リポジトリの CI のステータスを定期的に取得し、 DataDog に送ることで、 CI のステータスを監視するツールです。 現状は AWS Lambda で動かすことを想定していますが、他の方法でも動かせるようにするつもりです。
Motivation モチベーションは、 PR をマージしたあとに CI がこけた場合に通知が欲しいというものです。 マージしたあとに CI が一瞬で終わるなら無事終わるのを見届けてもいいんですが、 数分かかると待ってるのも時間がもったいないです。 しばらくしたあとに結果を確認すればいいんですが、それも面倒くさいですし、普通に忘れます。 そうするとデプロイしたつもりが実は CI がこけてたなんてことが普通にあります。
そういうことにすぐ気づけるよう、 Slack に通知がほしいと思っていました。
仕組み 仕組みは単純です。
GitHub API で各リポジトリのステータスを取得し、 DataDog API でステータスを送信しています。 DataDog API は Service Check API を使っています。 status は以下のようになります。
 0: 正常 1: 異常 3: ステータスの取得に失敗  また以下の tag が付きます。
 owner: リポジトリのオーナー repo: リポジトリ名 ref: ブランチ名  各リポジトリのステータスは現状 3 つをサポートしています。</description>
    </item>
    
    <item>
      <title>matchfile - 変更されたファイルの一覧から必要なタスクを導出するための CLI ツール</title>
      <link>https://techblog.szksh.cloud/matchfile/</link>
      <pubDate>Tue, 27 Oct 2020 19:39:44 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/matchfile/</guid>
      <description>自作の CLI ツール matchfile について紹介します。
https://github.com/suzuki-shunsuke/matchfile
この記事の執筆時点で最新バージョンは v0.1.1 です。
変更されたファイルの一覧から実行する必要のあるタスクを導出するための CLI ツールです。 Go で書かれていて、バイナリをダウンロードしてくれば使えます。
Pull Request (以下 PR) の CI では PR で変更されたファイルに応じて 必要なタスク(build, test, lint, etc) だけを実行したかったりします。
そこで、 PR で変更されたファイルパスのリスト と タスクが依存するファイルパスの条件 を元に、そのタスクを実行する必要があるか判定するためのコマンドとして matchfile を開発しました。
ただし、 matchfile の機能としては PR や CI とは独立しているので、もっと別の目的でも使えるとは思います。
matchfile は PR で変更されたファイルパスのリスト や タスクが依存するファイルパスの条件 を取得したりする機能はありません。
PR で変更されたファイルパスのリスト は ci-info という自分が作った別のツールを使うと取得できます。
タスクが依存するファイルパスの条件 はタスクに大きく依存するので matchfile はカバーしていません。
matchfile の使い方としては
$ matchfile run &amp;lt;PR で変更されたファイルパスのリストが書かれたファイルへのパス&amp;gt; &amp;lt;タスクが依存するファイルパスの条件が書かれたファイルへのパス&amp;gt; で、 PR で変更されたファイルパスのリスト のうち一つでも タスクが依存するファイルパスの条件 にマッチすれば true を、マッチしなければ false を標準出力します。 コマンドの exit code で結果を表現することも考えられましたが、そうすると set -e しているときに若干面倒くさいので、標準出力で表現しました。</description>
    </item>
    
    <item>
      <title>なぜ buildflow を作ったのか</title>
      <link>https://techblog.szksh.cloud/buildflow-goal/</link>
      <pubDate>Sun, 18 Oct 2020 09:53:08 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-goal/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書きます。
この記事では なぜ buildflow を作ったのかについて説明します。 開発者である自分の好みや置かれた環境などが所々に反映された内容になっています。
解決したい課題 自分は CI/CD の DX の改善に業務として取り組んでいます。 リポジトリはたくさんあり、横断的にメンテナンスしています。 幾つかのリポジトリはモノレポになっており、 CI の複雑さが増していたり、 CI の実行時間が長かったりします。
現在の CI/CD には以下のような問題があると感じています(他にもあるんですが、 buildflow と関係ないので割愛)。
 実行時間が長い  PR とは関係ない処理(test, build, etc) が実行されている   金銭的に高い  実行時間が長いので無駄にお金がかかっている CI サービスによっては並列度を上げることで実行時間が縮む場合があるが、それでもその分お金がかかる   PR とは直接関係ないところで失敗する  PR とは関係ない処理(test, build, etc) が実行されていて、それらが flaky で失敗する   メンテナンス性が悪い  属人化気味 何をやっているのか分かりにくい   同じような機能を複数のリポジトリで実装・メンテしたくない  これらの問題を解決するために buildflow を開発しました。
buildflow で必要な処理だけを実行する buildflow では PR の情報を自動で取得し、それらに応じて実行する処理を変更できます。 変更されたファイルに応じてだけでなく、 label や PR の author などでも変更できます。 Tengo script を用いて柔軟なロジックを実装できます。 JSON や YAML の読み込みもサポートしているので、依存関係などの設定を別ファイルで管理することも出来ます。</description>
    </item>
    
    <item>
      <title>buildflow の実行結果の出力形式</title>
      <link>https://techblog.szksh.cloud/buildflow-result-output/</link>
      <pubDate>Sun, 18 Oct 2020 08:36:09 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-result-output/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書きます。
この記事では buildflow の実行結果の出力フォーマットなどについて説明します。
ちょっと出力はわかりにくいかもしれません。 改善したいと思いつつ、どうあるべきなのかまだ見えてないのでこんな感じになっています。
task の標準出力、標準エラー出力はリアルタイムで出力されます。 また、複数のタスクを並列実行できます。 複数のタスクのログをリアルタイムで出力すると当然混じるので、区別がつくように各行の prefix に timestamp | task name |  をつけて出力します。 それでも混じるとわかりにくいので、 phase が完了後に、 phase の全 task のログを混ざらないようにそれぞれ標準エラー出力します。 つまり同じログが 2 回出力されますが 2 回実行されているわけではないです。
============== = Phase: phase 名 = ============== 10:47:54UTC | task A | + /bin/sh -c echo hello # 実行されるコマンド 10:47:54UTC | task B | + /bin/sh -c echo foo 10:47:54UTC | task A | hello # コマンドの標準(エラー)出力 10:47:54UTC | task A | .</description>
    </item>
    
    <item>
      <title>buildflow が自動で取得する Pull Request の情報</title>
      <link>https://techblog.szksh.cloud/buildflow-pr-info/</link>
      <pubDate>Sun, 18 Oct 2020 08:13:48 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-pr-info/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書きます。
この記事では buildflow が自動で Pull Request (以下 PR) の情報を取得してくる機能について説明します。
この機能は GitHub のみサポートしています。 GitLab や BitBucket はサポートしていません。 これは単純に自分が GitHub しか使わないからです。
PR の CI では
 変更されたものだけテストする 特定の PR ラベルがついていたら実行する 特定のユーザーの PR だけ処理を変える(bot とか)  のように PR の情報に基づいて挙動を変えたくなったりします。
シェルスクリプトで GitHub API 叩いて情報とってきて jq でパースしてとか、頑張れば別にできるんですが、 毎回そういうコードを書きたくないなと感じていました。
なお、 PR の情報をとってくる機能はデフォルトで無効化されています(GitHub Access Token 必要ですしね)。 設定で pr: true を指定してください。
PR の情報をとってくるには、以下の情報が必要です。
 repository owner: 設定ファイルで owner を設定するか、自動取得。 owner を設定してある場合はそちらが優先される repository name: 設定ファイルで repo を設定するか、自動取得。 repo を設定してある場合はそちらが優先される pull request number: 自動取得 GitHub Access Token: 環境変数 GITHUB_TOKEN または GITHUB_ACCESS_TOKEN を指定してください  取得される情報 以下のパラメータがテンプレートや Tengo script に渡されます。</description>
    </item>
    
    <item>
      <title>buildflow ではなぜ Tengo を採用しているのか</title>
      <link>https://techblog.szksh.cloud/buildflow-why-tengo/</link>
      <pubDate>Sat, 17 Oct 2020 22:35:56 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-why-tengo/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書きます。
この記事では buildflow でなぜ Tengo を採用しているのかについて説明します。
https://github.com/d5/tengo
Tengo に関しては https://techblog.szksh.cloud/buildflow-1/ でも多少触れています。
なぜ Tengo を採用しているのかに関しては
 なぜスクリプト言語を採用しているのか なぜ他の言語ではなく Tengo なのか  の 2 つの観点で話します。
なぜスクリプト言語を採用しているのか 逆にスクリプト言語を採用しない方法としては、 YAML などで独自 DSL のようなものを定義する方法があります。 DSL と言うと大げさかもしれませんが、 AND, OR, NOT といった論理を YAML のようなデータ記述言語で表現しようと思うとそんな感じになると思います。
この方法は扱いたいロジックが単純なものに限られるのであれば問題ないですが、 より柔軟なロジックを表現したいとなった場合に、無理があります。
 どうやって表現すればいいのか自分で考えないといけない  どう頑張っても独自ルールになるため、ユーザーにとって直感的とは言えない   正しく実装しないといけない 仕様をドキュメント化しないといけない  一方、 Go では幾つかのスクリプト言語がサードパーティのライブラリとして実装されており、 buildflow のようなツールに組み込むことが出来ます。
https://github.com/avelino/awesome-go#embeddable-scripting-languages
これらを活用すれば上記の問題は解決できるうえに、非常に柔軟にロジックを実装できます(勿論言語によりますが)。
なぜ他の言語ではなく Tengo なのか 単純に https://github.com/avelino/awesome-go#embeddable-scripting-languages で紹介されているライブラリの中で一番要件にマッチしてそうだったからです。 といっても全てをちゃんとチェックしたわけではありませんが。 Lua とかもあるのでそれでも良かったかもですが、自分は Lua を全然知りません。 あとちゃんとバージョンニングされていたのも理由の一つです。 Tengo より人気のある言語もありましたが、バージョニングされてないという理由で見送ったりしました。</description>
    </item>
    
    <item>
      <title>buildflow の dynamic task</title>
      <link>https://techblog.szksh.cloud/buildflow-dynamic-task/</link>
      <pubDate>Sat, 17 Oct 2020 21:29:51 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-dynamic-task/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書きます。
この記事では buildflow の dynamic task という機能について説明します。 dynamic task では task.items の値でループを回し、複数の task を動的に生成できます。 勿論 task.items はオプションなので、指定しなければ普通の task として扱われます。 task.items を指定する場合、 map か list か、それらを返す Tengo script でないといけません。
--- phases: - name: main tasks: - name: &amp;#34;list {{.Item.Key}} {{.Item.Value.name}}&amp;#34; command: command: &amp;#34;echo {{.Item.Key}} {{.Item.Value.name}} {{.Item.Value.age}}&amp;#34; items: - name: foo age: 10 - name: bar age: 20 上記の設定は dynamic task を使わないとこうなります。
--- phases: - name: main tasks: - name: &amp;#34;list 0 foo&amp;#34; command: command: &amp;#34;echo 0 foo 10&amp;#34; - name: &amp;#34;list 1 bar&amp;#34; command: command: &amp;#34;echo 1 bar 20&amp;#34; パラメータ Item は Key, Value を持ち、 Items が map の場合、それぞれ map の key, value が渡され、 list の場合、 index と value が渡されます。</description>
    </item>
    
    <item>
      <title>buildflow の task の input, output という機能</title>
      <link>https://techblog.szksh.cloud/buildflow-input-output/</link>
      <pubDate>Sat, 17 Oct 2020 21:05:41 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-input-output/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書きます。
この記事では buildflow の task の input, output という機能について説明します。 task の input, output は Tengo script で task のパラメータを整形する機能です。
task の command.command や write_file.template など、幾つかの設定では Go の text/template が使えますが、 text/template は複雑なロジックを記述したりするのには向いていません。 そこで task の input で Tengo script を使って必要なデータの整形を行うことで、 template は比較的きれいな状態に保つことが出来ます。
これは MVC モデルで View とロジックを分離するみたいな考え方と似ているかもしれません。
output ではコマンドの実行結果を整形することが出来ます。 例えばコマンドの標準出力をユニークな文字列のリストにしたり出来ます。
task.input は task.when が評価されたあと、 task の command などが実行される前に評価されます。 つまり、 task.when や task.dependency で同じ task の input の結果を参照は出来ません。</description>
    </item>
    
    <item>
      <title>buildflow で設定ファイルを分割する</title>
      <link>https://techblog.szksh.cloud/buildflow-split-files/</link>
      <pubDate>Sat, 17 Oct 2020 20:49:28 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-split-files/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書こうかなと思います。
この記事では buildflow の設定ファイルを分割する方法について説明します。
buildflow では一部の設定項目について他のファイルのパスを指定して読み込むということが出来ます。 1 つのファイルに全部の設定を書いていると、ファイルが大きくなってメンテナンス性が悪くなったり、 コードオーナーが曖昧になったりするので、そういう場合は分割すると良いでしょう。 コードオーナーが異なる複数のサービスで共通の設定ファイルを用いる場合、ファイルを分割して GitHub の CODEOWNERS を設定するのもよいでしょう。 あまりないかもしれませんが、ファイルを分割すると同じファイルを読み込んで再利用も出来ます。
また、 Tengo script を独立したファイルに分割すると、 test が可能になります。 Tengo script をテストするためのツールとして tengo-tester というツールも開発しているので、そちらをお使いください。
以下のようなファイル読み込みの設定があります。
 phase.import task.import: task.input_file task.output_file task.when_file command.command_file command.env[].value_file write_file.template_file  ファイルのパスは、絶対パスか、実行中の build の設定ファイルが存在するディレクトリからの相対パスになります。</description>
    </item>
    
    <item>
      <title>buildflow の script や template に渡される parameter</title>
      <link>https://techblog.szksh.cloud/buildflow-parameter/</link>
      <pubDate>Sat, 17 Oct 2020 19:44:39 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-parameter/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書こうと思います。
この記事では buildflow の Tengo script やテンプレートにパラメータとして渡される変数について紹介します。
buildflow では Tengo script はテンプレートが使える設定項目が多くあります。それらの設定には共通のフォーマットのパラメータが渡されます。
 PR: Pull Request の情報: GitHub API のレスポンス body Files: Pull Request で更新されたファイルの一覧: GitHub API のレスポンス body Phases: 対象の Phase よりも前の Phase の結果 Phase: 対象の Phase Tasks: 対象の Phase の Task の結果 Task: 対象の Task Item: dynamic task のパラメータとして渡される Meta: 設定 meta  Phase
 Status: Phase の実行結果  succeeded failed skipped   Tasks: Phase の task の実行結果 Meta: phase の 設定 meta  Task</description>
    </item>
    
    <item>
      <title>buildflow の task の設定項目</title>
      <link>https://techblog.szksh.cloud/buildflow-task/</link>
      <pubDate>Sat, 17 Oct 2020 18:26:57 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-task/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書こうと思います。
この記事では buildflow の task の基本的な設定項目などについて説明します。 数が多いので、個々の設定の詳細はまた別の記事に書きます。
task には幾つか type がありますが、全ての type に共通するパラメータが以下になります。
 name: task 名。 unique である必要はない。 Go の text/template が使える when: task を実行するか否か。 真偽値か Tengo script  when_file で外部ファイルを読み込める   dependency: task の依存関係の定義。 task 名のリストか、 Tengo script items: dynamic task の設定。 loop を使って複数の task を動的に生成できる  任意の list か map か、 Tengo script   input: Tengo script で task のコマンドのパラメータを生成できる  input_file で外部ファイルを読み込める   output: Tengo script で task の実行結果を整形できる。他の task が参照して挙動を変えたりできる  output_file で外部ファイルを読み込める   meta: ユーザーが自由にパラメータを定義できる map  上記の設定は name 以外はオプションです。</description>
    </item>
    
    <item>
      <title>buildflow の build, phase, task について</title>
      <link>https://techblog.szksh.cloud/buildflow-build-phase-task/</link>
      <pubDate>Sat, 17 Oct 2020 18:09:19 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-build-phase-task/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書こうと思います。
この記事では buildflow の概念である build, phase, task について書きたいと思います。
buildflow には Build, Phase, Task という概念があります。 CircleCI の Pipeline, Workflow, Job みたいなものと思ってもらえるとよいと思います。
$ buildflow run で 1 つの build が実行されます。 build は複数の phase からなり、 phase が 1 つずつ順に実行されます。 phase は複数の task からなり、 task が全て終了すると、その phase も終了となります。 task は並列に実行したり、依存関係を定義したりできます。 task では外部コマンドを実行したりできます。
設定ファイルでは phases, tasks をそれぞれ配列で指定します。
--- phases: - name: setup tasks: - name: hello command: command: echo hello - name: foo command: command: echo foo - name: build tasks: - name: hello command: command: echo hello - name: foo command: command: echo foo dependency: - hello - name: post build tasks: - name: hello command: command: echo hello 上の例では 3 つの phase setup, build, post build が順に実行されます。 デフォルトではどれかの phase が失敗するとそれ以降の phase は実行されません(この挙動は変えられます)。</description>
    </item>
    
    <item>
      <title>buildflow での Tengo の使い方</title>
      <link>https://techblog.szksh.cloud/buildflow-tengo/</link>
      <pubDate>Sat, 17 Oct 2020 17:18:34 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-tengo/</guid>
      <description>buildflow というツールを開発しているので buildflow というタグをつけて何回かに分けてブログを書こうかなと思います。
この記事では buildflow で Tengo というスクリプト言語をどのように使っているか書きたいと思います。
https://github.com/d5/tengo
buildflow の設定では task.when や task.dependency, task.input などで Tengo script が使えますが、 1 つの共通のルールがあります。 result という変数を宣言し、 script の実行結果をその変数に持たせるというルールです。 これは Tengo の仕様とかではなく、 buildflow 特有のルールです。 もっとも単純な例だと次のような感じです。
result := true task.input, output などだと result の値が Task.Input, Task.Output として参照できるようになります。
--- phases: - name: main tasks: - name: hello input: |result := { foo: &amp;#34;bar&amp;#34; } command: command: &amp;#39;echo &amp;#34;{{.Task.Input.foo}}&amp;#34;&amp;#39; when: &amp;#34;result := true&amp;#34; Tengo の標準ライブラリ Tengo には標準ライブラリがあります。 buildflow では全ての標準ライブラリが使えます。</description>
    </item>
    
    <item>
      <title>buildflow というワークフローエンジンのようなタスクランナーのようなツールを作っている</title>
      <link>https://techblog.szksh.cloud/buildflow-1/</link>
      <pubDate>Sat, 17 Oct 2020 16:06:31 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/buildflow-1/</guid>
      <description>buildflow というツールを開発しているので紹介します。 buildflow というタグをつけて何回かに分けてブログを書こうかなと思います。 1本目のこの記事では
 どんなツールか Hello World 特徴  について簡単に説明します。
どんなツールか https://github.com/suzuki-shunsuke/buildflow
ワークフローを実行するための CLI ツールです。 ワークフローエンジンと言うと Airflow とか Azkaban, Argo Workflows のようなツールをイメージするかと思いますが、 それらとは目的も機能も違います。 一部の CI サービスではワークフローのローカル実行をサポートしてたりしますが、そんなイメージで良いかもしれません。 buildflow では task と task の依存関係を設定ファイルに定義し、コマンドを実行するとローカルでタスクが実行されます。 そういうとタスクランナーといったほうがいいのかもしれませんが、個別のタスクを指定して実行するような機能はないので、タスクランナーとも違う気がします。
CI サービス上で実行することを目的として開発しています(汎用的なツールなので他の目的でも使えるとは思います)。
Hello World まだどんなツールかピンと来てない人もいるかもしれないので、簡単な Hello World をやってみましょう。
GitHub Releases からバイナリをダウンロードしてください。
次のような設定ファイル .buildflow.yaml を用意します。
--- phases: - name: main tasks: - name: hello command: command: echo hello 次のコマンドを実行すると task が実行されます。
$ buildflow run ============== = Phase: main = ============== 07:50:46UTC | hello | + /bin/sh -c echo hello 07:50:46UTC | hello | 07:50:46UTC | hello | hello 07:50:46UTC | hello | ================ = Phase Result: main = ================ status: succeeded task: hello status: succeeded exit code: 0 start time: 2020-10-17T07:50:46Z end time: 2020-10-17T07:50:46Z duration: 4.</description>
    </item>
    
    <item>
      <title>tfnotify の parse error を通知する</title>
      <link>https://techblog.szksh.cloud/post-tfnotify-parse-error/</link>
      <pubDate>Sat, 12 Sep 2020 08:05:59 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/post-tfnotify-parse-error/</guid>
      <description>tfnotify が terraform の標準出力のパースに失敗してコメントを投稿できないことがあります。
コメントを投稿できなくてもビルドのログには残るのですが、やはりコメントを投稿できると便利なので、tfnotify がパースエラーでコメントの投稿に失敗したら、 github-comment でコメントを投稿するようにしました。
なお、この記事を書いている時点のバージョンは tfnotify v0.7.0, github-comment v1.9.0 です。
例えば tfnotify plan がパースエラーになった場合、 cannot parse plan result というメッセージが標準エラー出力されます。 そこで標準エラー出力に cannot parse plan result が含まれていたら github-comment でコメントするようにします。
terraform plan | github-comment exec -k plan -- tfnotify plan .github-comment.yml
# 細かく template を分けているが、別に分けなくてもよい templates: # header は CodeBuild の場合 header: &amp;#39;{{Env &amp;#34;TARGET&amp;#34;}} [Build link]({{Env &amp;#34;CODEBUILD_BUILD_URL&amp;#34;}})&amp;#39; exit_code: &amp;#39;:{{if eq .ExitCode 0}}white_check_mark{{else}}x{{end}}: Exit Code {{.ExitCode}}&amp;#39; join_command: |``` $ {{.JoinCommand}} ``` hidden_combined_output: |&amp;lt;details&amp;gt; &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;{{.</description>
    </item>
    
    <item>
      <title>AWS CodeBuild 良さそう</title>
      <link>https://techblog.szksh.cloud/codebuild-1/</link>
      <pubDate>Sun, 02 Aug 2020 13:21:37 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/codebuild-1/</guid>
      <description>AWS CodeBuild を検証しているんですが、結構良いですね。 現状 Jenkins や CircleCI で実行しているジョブや CI/CD を一部移行したいなと思いました。
一部と言っているのは、単純に全部いきなり移行するのは難しいから共存する前提で考えるくらいの意味です。
なお、これを書いている時点ではまだ軽く検証しているだけなので、CodeBuild の理解は浅いです。
特徴としては
 Managed AWS 以外のサービスに AWS の credential を登録しなくて良い  Secret を AWS Secrets Manager で管理できる Secret を至るところに設定するのではなく、 AWS Secrets Manager か何かで一元管理するのが理想 Role 作って Build Project の Service Role として指定するだけなら credential を扱う必要がなくて楽で安心   VPC 内で実行できる GitHub 連携も簡単  Webhook の設定で PR の細かなイベント(merge とか reopen とか)に対応しているのも良い PR を merge したときも PR で変更されたファイルによって Webhook をフィルタできるのが良い   変更されたファイルによって実行するBuild Project を変更するようなロジックを実現できる(monorepo で特に有効)  といった点が挙げられます。</description>
    </item>
    
    <item>
      <title>github-comment - GitHub にコメントを投稿する CLI</title>
      <link>https://techblog.szksh.cloud/github-comment/</link>
      <pubDate>Fri, 31 Jul 2020 20:42:54 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/github-comment/</guid>
      <description>GitHub の issue や pull request, commit にコメントを投稿する CLI ツールを作りました(結構前の話ですが)。
https://github.com/suzuki-shunsuke/github-comment
このブログの執筆時点で最新は v1.5.0 です。
Go 製なので、 GitHub Releases からダウンロードしてくれば簡単にインストールできます。
想定している主な用途は、 CI/CD の 結果をコメントで通知することで DX を向上することです。 例えば CI がこけたらこけたコマンドとエラーメッセージを通知するなどです。
github-comment には
 init: 設定ファイルの雛形を生成する post: コメントを投稿する exec: 外部コマンドを実行し、その結果を元にコメントを投稿する  という 3 つのサブコマンドがあります。
コメントの投稿には GitHub の Access Token が必要です。 コマンドライン引数 -token でも渡せますが、環境変数として設定しましょう。
$ export GITHUB_TOKEN=xxx # GITHUB_ACCESS_TOKEN も可 post コマンド こんな感じでコメントを投稿できます。
$ github-comment post -org suzuki-shunsuke -repo github-comment -pr 1 -template test パラメータの数が多いですが、いくつかの Platform では環境変数から自動でパラメータを補完してくれます。</description>
    </item>
    
    <item>
      <title>clap - 簡単にツールをインストールするためのツールを作った</title>
      <link>https://techblog.szksh.cloud/clap/</link>
      <pubDate>Mon, 06 Jul 2020 16:52:58 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/clap/</guid>
      <description>多分車輪の再生産だとは思いますが、簡単にツールをインストールするための CLI ツールを作りました。 tarball や zip をダウンロードして展開して指定したパスにインストールするツールです。
https://github.com/suzuki-shunsuke/clap
Go で書かれています。 ツールの名前(clap)には特別な意味や理由はなく、なんとなくです。
CI で何かしらのツールをインストールすることがままあって、そのためのシェルスクリプトを都度書くのが割と面倒なのでツール化しました。
このブログを書いている時点でバージョンは v0.1.0-1 で、最低限の機能しかありませんが、9割型ニーズを満たせるかなと思います。
使い方は以下のようになっています。
$ clap &amp;lt;URL&amp;gt; &amp;lt;インストールするファイルのアーカイブ内での相対パス&amp;gt;:&amp;lt;インストール先&amp;gt; [&amp;lt;インストールするファイルのアーカイブ内での相対パス&amp;gt;:&amp;lt;インストール先&amp;gt;...] 例えば conftest を /usr/local/bin にインストールする場合次のようになります。
CONFTEST_VERSION=0.18.2 clap install https://github.com/instrumenta/conftest/releases/download/v${CONFTEST_VERSION}/conftest_${CONFTEST_VERSION}_Linux_x86_64.tar.gz conftest:/usr/local/bin/conftest chmod a+x /usr/local/bin/conftest パーミッションの付与はやってくれないので必要に応じてやってください。 ファイルの圧縮形式は URL から自動で判別してくれます。
上記の conftest のインストールを今までは次のようなシェルスクリプトを書いていました。
#!/usr/bin/env bash  set -eu CONFTEST_VERSION=0.18.2 dirpath=$(mktemp -d) pushd &amp;#34;$dirpath&amp;#34; TARFILE=conftest_${CONFTEST_VERSION}_Linux_x86_64.tar.gz curl -OL https://github.com/instrumenta/conftest/releases/download/v${CONFTEST_VERSION}/${TARFILE} tar xvzf $TARFILE mv conftest /usr/local/bin/conftest chmod a+x /usr/local/bin/conftest popd rm -R &amp;#34;$dirpath&amp;#34; 地味に面倒ですね。これをツール毎に書いて、しかも圧縮形式によって微妙に変えないといけません。</description>
    </item>
    
    <item>
      <title>CircleCI の run の実行時間を DataDog に送る</title>
      <link>https://techblog.szksh.cloud/send-circleci-run-execution-time-to-datadog/</link>
      <pubDate>Sat, 23 May 2020 14:20:53 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/send-circleci-run-execution-time-to-datadog/</guid>
      <description>小ネタです。
dd-time を使って CircleCI の run を使ったコマンドの実行時間をどう計測したらいいのかちょっと考えました。
以前、コマンドの実行時間を DataDog に送るツール dd-time を作りました。
 https://techblog.szksh.cloud/dd-time/ https://github.com/suzuki-shunsuke/dd-time  これは基本的に以下のように引数として -- 以降に実行するコマンドを指定します。
$ dd-time -m dd_time.execution_time -t command:docker-build -- docker build . 実行するスクリプトを標準入力で渡したい場合はこうします。
$ curl https://example.com/install.sh | dd-time -m dd_time.execution_time -- sh もちろんシェルスクリプトである必要はなくて例えば Python だったらこうなります。
$ curl https://example.com/setup.py | dd-time -m dd_time.execution_time -- python CircleCI の run では shell オプションで shell を指定できます。
https://circleci.com/docs/2.0/configuration-reference/#run
なので command 全体の時間を計測したい場合は、 shell を次のようにします。
- run: name: test dd-time shell: /usr/local/bin/dd-time -m dd_time.</description>
    </item>
    
    <item>
      <title>CircleCI の checkout の注意点</title>
      <link>https://techblog.szksh.cloud/circleci-checkout-default-branch/</link>
      <pubDate>Fri, 24 Apr 2020 16:53:01 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/circleci-checkout-default-branch/</guid>
      <description>CircleCI の組み込みの command checkout の注意点について書きます。
なお、ここに書かれている内容は 2020/04/24 時点のものであり、予告なしに checkout の挙動が変わる可能性があります。
また、今回は話を簡略化するため、 checkout 実行時点で .git がない(つまりキャッシュしていない)ものとします。
最初に結論 先に結論を書くと
 CircleCI ではローカルのデフォルトブランチを参照しないほうが良い($CIRCLE_BRANCH がデフォルトブランチである場合は除く)  履歴が origin と異なり、 $CIRCLE_BRANCH と同様になっているため   代わりに origin のデフォルトブランチを参照したほうが良い git branch -f &amp;lt;デフォルトブランチ&amp;gt; origin/&amp;lt;デフォルトブランチ&amp;gt; を実行してデフォルトブランチの履歴を修正するのもあり  checkout がなにをやっているか checkout でなにをやっているかは実際に使ってみて CircleCI の job の詳細画面(?) から確認できます。
サンプル: https://app.circleci.com/pipelines/github/suzuki-shunsuke/test-circleci/73/workflows/5611059c-d6b1-4a34-91b5-45d6f149d408/jobs/96
ここでは checkout の全てについては触れません。一部抜粋します。
elif [ -n &amp;#34;$CIRCLE_BRANCH&amp;#34; ] then git reset --hard &amp;#34;$CIRCLE_SHA1&amp;#34; git checkout -q -B &amp;#34;$CIRCLE_BRANCH&amp;#34; fi git reset --hard などをしています。</description>
    </item>
    
    <item>
      <title>Skaffold で特定のサービスだけ動かすためのツールを作った</title>
      <link>https://techblog.szksh.cloud/skaffold-generator/</link>
      <pubDate>Sun, 05 Apr 2020 18:53:25 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/skaffold-generator/</guid>
      <description>自作の CLI ツール skaffold-generator の紹介です。 プロトタイピングみたいなノリで半日くらいで割と手早く作れました。 名前が長くて適当なのでもっと良い名前ないかなと思ってます。
Skaffold に欲しい機能がないので補完する感じで作ったのですが、「それ〇〇で出来るよ」とかあったら(GitHub issue とか Twitter で)教えていただけると幸いです。
どんなツールか 設定ファイル skaffold-generator.yaml を監視して変更があったら skaffold.yaml を生成するツールです。設定ファイルでサービスの依存関係を定義できたり、コマンドライン引数で指定したサービス及びそれが依存するサービスに関連した設定だけを使って skaffold.yaml を更新します。 このツールは skaffold.yaml を生成するだけなので実際にアプリケーションをビルド・デプロイするには skaffold と組み合わせて使います。
なぜ作ったか 元々ローカルでアプリケーションを動かしながら開発するために Docker Compose を使ってるリポジトリがあるのですが、それを skaffold に移行出来ないか検証しています。 まだ skaffold を触り始めたばかりで理解が浅いのですが、 本番環境は k8s で動いてるからローカルも k8s で動かせるといいかなと思ったり、あとは変更を検知して自動でビルド・デプロイしてくれたりして便利そうかなと思いました。 まぁ結果的に移行しないことになったとしても、 Skaffold と現状の仕組みについて理解が深まればいいかなくらいのつもりです。
検証の過程で、 以下のようなことが Docker Compose だと出来るけど Skaffold だと難しそうだと思いました。
 サービスの依存関係を定義すること  Skaffold というより k8s の問題かとは思いますが Docker Compose だと依存するものを自動で起動してくれて便利   コマンドライン引数で指定したサービスだけ起動すること  Skaffold だと skafffold.yaml で定義したものすべてがビルド・デプロイされるという認識    サービスの数が少なければ全部ビルド・デプロイでもいいですが、 マイクロサービスをモノレポで管理しているような場合、 すべてのマイクロサービスをビルド・デプロイするのは無駄が大きかったりします。</description>
    </item>
    
    <item>
      <title>Terraform ハンズオン with MySQL Provider</title>
      <link>https://techblog.szksh.cloud/terraform-hands-on-with-mysql-provider/</link>
      <pubDate>Fri, 17 Jan 2020 09:14:08 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/terraform-hands-on-with-mysql-provider/</guid>
      <description>Terraform を勉強するには実際に使ってみるのが一番手っ取り早いですが、 では手頃な題材はあるかと言われると少し難しいです。
公式の Getting Started では AWS が使われていますが、 AWS のアカウントやクレデンシャルが必要ですしお金もかかってしまいます(無料枠はありますが)。 もう少し手軽なものが欲しいところです。
そこで公式の Provider で丁度いいものはないか探したところ、 MySQL Provider が良さそうでした。 MySQL のユーザーや Database を Terraform で管理したいとは自分は思いませんが、 Terraform の入門で遊ぶにはちょうどよいでしょう。
ちなみに公式の Provider のリストはこちらです。
 https://github.com/terraform-providers https://www.terraform.io/docs/providers/index.html  また、 Terraform に関しては Terraform 入門 も参照してください。
今回の作業用に適当にディレクトリを作成し、そこで作業しましょう。
以降、コマンドの実行結果は一部省略することがあります。
$ mkdir workspace $ cd workspace Terraform のバージョンと tfenv Terraform を複数人で使う場合、 Terraform のバージョンを揃えるのが重要です。 理由の一つとして、 Terraform の State は State を作成した Terraform のバージョンを記録しており、それより古いバージョンの Terraform で terraform plan などを実行すると失敗するようになっていることが挙げられます(この点については後でも触れます)。 そういう意味では、 tfenv によってバージョン管理するのが良いです。</description>
    </item>
    
    <item>
      <title>Terraform 入門</title>
      <link>https://techblog.szksh.cloud/terraform-getting-started/</link>
      <pubDate>Thu, 16 Jan 2020 09:25:05 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/terraform-getting-started/</guid>
      <description>参考  10分で理解するTerraform | Qiita Terraform入門資料(v0.12.0対応) ~基本知識から設計や運用、知っておくべきtipsまで~ | Qiita AWSでTerraformに入門 | Developers.io Terraform職人入門: 日々の運用で学んだ知見を淡々とまとめる | Qiita  手を動かしたい方は Terraform ハンズオン with MySQL Provider も参考にしてください。
前提  執筆時点 (2020/01/05) で Terraform の最新バージョンは v0.12.18 です  Terraform とは Terraform は Infrastructure as Code を実現する汎用的なCLIツールです。 インフラの状態を設定ファイルに定義し、コマンドを実行することで、 実際のインフラの状態と設定ファイルの差分を検知し、設定ファイルに記述されたとおりになるようにインフラを変更(CRUD)します。
Hashicorp という企業がホストしている OSS になります。 Go で書かれています。 https://github.com/hashicorp/terraform
Terraform のインストール Terraform は Go 製なので 1 バイナリをダウンロードしてインストールするだけです。
https://www.terraform.io/downloads.html
tfenv を使うと管理が楽です。
https://github.com/tfutils/tfenv
tfenv は Terraform のバージョン管理ツールです。 pyenv や rbenv の Terraform 版みたいなものです。</description>
    </item>
    
    <item>
      <title>Terraform の State Locking の概要</title>
      <link>https://techblog.szksh.cloud/terraform-state-locking/</link>
      <pubDate>Fri, 10 Jan 2020 16:18:05 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/terraform-state-locking/</guid>
      <description>Terraform の State Locking という機能の概要について説明します。 ただし、自分もちゃんと理解しているわけではないので、推測も混じります。 基本的には公式ドキュメントに書いてある内容なのでそちらをご参照ください。
State Locking とは terraform plan などのコマンドは State を変更する場合があります。 その処理は atomic ではないため、同時に複数のコマンドが State を書き換えようとすると不整合が生じる可能性があります。
 例えば S3 backend の state を state rm で更新する場合を考えます。 これはコマンド内部で
 現在の State を取得する (READ) 修正した State を S3 に push する (WRITE)  という処理を行っているはずであり、複数のコマンドを実行した場合、READ と WRITE の間に他のコマンドによって WRITE されると、その WRITE による変更が消えてしまいます。
 そこで State Locking を使うと各コマンドで State を変更する前に lock を取り、WRITE 後に lock を解除します。
コマンドラインオプション plan, apply, refresh, state rm, state mv, state push には次のようなオプションがあります。</description>
    </item>
    
    <item>
      <title>dd-time - コマンドの実行時間を Datadog に送るツール</title>
      <link>https://techblog.szksh.cloud/dd-time/</link>
      <pubDate>Sat, 30 Nov 2019 13:54:47 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/dd-time/</guid>
      <description>コマンドの実行時間を Datadog に送る dd-time というツールを作りました。
このツールは circle-dd-bench にインスパイアされていますが、 CircleCI 以外でも需要あると思ったり、他にも幾つか改善したい部分があったので自作することにしました。
circle-dd-bench については circle-dd-bench の作者が書いたブログ https://blog.yuyat.jp/post/circle-dd-bench/ も参考にしてください。
dd-time は Go 製なので GitHub Releases からバイナリをダウンロードしてインストールすれば使えます。
使い方はシンプルで実行時間を計測したいコマンドの前に dd-time -- をつけるだけです。 例えば Docker image のビルドの時間を計測したい場合次のような感じになります。
$ dd-time -t command:docker-build -- docker build . Datadog の API key を環境変数 DATADOG_API_KEY として設定する必要があります。 こうすると Datadog の Post timeseries points API を使い、command_execution_time というメトリックス名(変更可能)でコマンドの実行時間が送られます。
メトリックスの名前や host, tags はそれぞれ --metric-name (-m), --host, --tag (-t) で指定できます。 --tag は複数回指定可能で、 key:value というフォーマットで指定します。
CircleCI で実行した場合、 CircleCI のビルドイン環境変数が tag として勝手に設定されますが、 CircleCI 以外でも使えます。</description>
    </item>
    
    <item>
      <title>モブレビューやっていきたい</title>
      <link>https://techblog.szksh.cloud/mobreview/</link>
      <pubDate>Sun, 10 Nov 2019 18:31:05 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/mobreview/</guid>
      <description>最近モブレビュー取り入れたいと感じていて、なんで取り入れたいかなどについて書いてみました。 モブレビュー自体まだ 2, 3 回しかやってないので説得力にかけますが、ご容赦ください。
想定  チームは 6 人以下 Pull Request (以下 PR) をマージするには必ず他の誰かが approve しないといけない  目的  チーム内の情報共有  属人化の解消 退職や異動などによる情報の喪失(誰も分からない状態)を防ぐ チーム外の人とのコミュニケーションにも活用できる   レビュー待ちの短縮 レビューの品質の改善 仕事を評価してもらうことで承認欲求を満たす オンボーディングの改善  レビューを通じて必要な知識を吸収してもらう オンボーディングに限ったことではないが、オンボーディングにも有効ではないか   最後までやりきる  箇条書した内容を補足します。
情報共有 属人化の解消は結構重要だと自分は思っていて、特定の人じゃないと出来ないこと、わからないことというのは ボトルネックや技術的負債だったり、障害対応時に致命的になりかねません(深夜に障害が起こってAさんに聞かないとわからないのに、Aさんと連絡が取れないとか最悪)。 チーム全員とはいかなくても 3 人ぐらいは出来る・分かってる必要があるかなと思います。
まぁ、当初は 3 人ぐらい分かってても、時間が経って異動やら退職やらで気づいたら分かっている人いなくなってたというのは ありえなくないので、そういったリスクをどうやって防ぐかというのは考える必要がありますが、今回の話とは外れるので割愛します。
また、チーム外の人とのコミュニケーションにも活用できると思っていて、 例えばランチや飲み会でチーム外の人とのコミュニケーションを取る際に自分のチームの他の人が対応した件とかが話題に上がったときに ちゃんと情報共有できてないと、自分は担当外なので分かりませんとなってしまうでしょう。 知っていればむしろ自分から話題にできるかもしれません。 そこから発展して更に新しいタスクの話もできるかもしれません。
レビューの改善 せっかくいい仕事をしても中々レビューしてもらえないとなると不満がたまります。 モブレビューを実施することでレビューを促進し、レビュー待ちの時間を短くできることを期待します。 レビューしたくても良くわからなくてレビューできないというパターンもあると思うので、 わからない部分をモブレビューで解消されてレビューが進むと良いですね。
また、ちゃんと複数人でレビューすることで目先の問題を解決するだけの本質的でない問題解決を防ぐことが出来ることもあると思います。 「いや、それそもそもPRの前提がおかしくない？前提となっている仕様を見直すべきなのでは」 みたいなこともあるかもしれません。
仕事を評価してもらうことで承認欲求を満たす いい仕事したら他の人にも知ってもらいたいというのは自然なことでしょう。 いい仕事を褒めるのは良いチーム・環境であり働きやすさというところにつながるのではないでしょうか。
ちなみに現職だと Slack で他のチームにも共有して emoji で褒め称えるというのが結構やられていて 気持ちのいい環境だなと思っています。</description>
    </item>
    
    <item>
      <title>go-timeout - command の timeout</title>
      <link>https://techblog.szksh.cloud/go-timeout/</link>
      <pubDate>Mon, 04 Nov 2019 10:00:21 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/go-timeout/</guid>
      <description>作ったのは 2ヶ月くらい前の話ですが、 Go の command の timeout を実装するためのライブラリを作ったので紹介します。
https://github.com/suzuki-shunsuke/go-timeout
基本的には https://github.com/Songmu/timeout をオススメしますが、これだと上手くいかないパターンがあったので自作しました。
Go の command の timeout に関しては https://junkyard.song.mu/slides/gocon2019-spring/#24 がとても参考になります。
上記のスライドでは
 標準ライブラリの exec.CommandContext でも停止できるが、 SIGKILL で強制的に停止することになる  子プロセスが停止しない   公式見解 では、SIGKILL 以外は標準ライブラリではサポートしない。サードパーティでやればよい Songmu/timeout 使えば SIGKILL 以外でより安全に停止できる  ということが丁寧に説明されています。
自分は cmdx という task runner を開発していてその中で task の実行時に timeout を設定出来るようにしました。 当初 Songmu/timeout を使って実装したのですが、問題があることに気づきました。 それは、 command の中で fzf を使うと、上手く動かないというものでした。
 https://github.com/suzuki-shunsuke/cmdx/issues/52 https://twitter.com/szkdash/status/1165529415238815745  正直この辺の挙動はちゃんと理解できていないのですが、 調べてみると Songmu/timeout だと syscall.SysProcAttr の Setpgid を true に設定していて、そうすると fzf が上手く動かないようでした。</description>
    </item>
    
    <item>
      <title>cmdx - task runner</title>
      <link>https://techblog.szksh.cloud/cmdx/</link>
      <pubDate>Fri, 23 Aug 2019 11:35:13 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/cmdx/</guid>
      <description>最近自作した OSS, cmdx の紹介です。
https://github.com/suzuki-shunsuke/cmdx
cmdx は task runner です。
task runner の定義はググってもわからなかったので、 cmdx を task runner と呼ぶのが適切かわかりませんが、 ここではプロジェクト固有のタスク
 依存するライブラリのインストール ビルド テスト コード整形 lint etc  などを管理するものとします。
類似するものとしては以下のようなものがあります。
 Make npm scripts Task tj/robo mumoshu/variant  使い方 詳細は README を読んでください。
$ cmdx -i で設定ファイルの雛形を生成します。
そして設定ファイルに task を定義していきます。 設定に関しては README を参照してください。
そうすると cmdx -l でタスクの一覧とその説明が見れます。
例えば次は cmdx のリポジトリでの実行結果です。
$ cmdx -l init, i - setup git hooks coverage, c - test a package (fzf is required) test, t - test fmt - format the go code vet, v - go vet lint, l - lint the go code release, r - release the new version durl - check dead links (durl is required) ci-local - run the Drone pipeline at localhost (drone-cli is required) これにより新しくプロジェクトに参画した人もどのような task があるのか直ぐわかります。 例えば test を実行したければ cmdx t を実行すればいいことがわかります。 cmdx help test とすればここのタスクのより詳細なヘルプが見れます。</description>
    </item>
    
    <item>
      <title>Drone Extension のリスク</title>
      <link>https://techblog.szksh.cloud/risk-of-drone-extension/</link>
      <pubDate>Thu, 15 Aug 2019 07:54:23 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/risk-of-drone-extension/</guid>
      <description>Drone v1 では Extension という仕組みが導入されました。
これは文字通り Drone を拡張する仕組みで、仕様に従って作れば自由に Drone を拡張できます。
https://docs.drone.io/extensions/overview/
全てを本体でやるのではなく、拡張する仕組みを提供し、あとはコミュニティに委ねるというのが Drone の一つの方針とも言えると思います。
Extension は非常に面白い仕組みだと思いますが、 Drone を運用する立場からすると中々頭が痛い仕組みな気がしてて、 自分は導入に対し慎重な立場です。 単なる杞憂で済めば良いのですが、その懸念について書きたいと思います。
根本は Drone Extension 固有の問題と言うより、一般的な拡張機構全般に言えることだと思います。 ただし、 Drone Extension は全てのビルドに影響を及ぼす、 CI/CDシステムが動かなくなるとサービスのリリースに影響を及ぼしかねないということからよりリスクの高いものになっています。
 本体の drone/drone と比べ、開発は活発ではなく、サードパーティの extension はいつ開発が止まってもおかしくない 本体の drone/drone と比べ、ドキュメントやサポート体制が貧弱だと思われる(drone に関しては https://discourse.drone.io でサポートされているが、サードパーティの extension では難しい) ユーザーからの extension に関する要望を受け付けるようになると、管理者の負担になる extension のクォリティはマチマチであり、例外処理が甘かったり、ちゃんとエラーを吐かないものもあるだろう トラブルシューティングが難しいと思われる extension の仕組み上、extension を必要としないビルドにも影響を及ぼしうる 一度追加し、依存しだすと消すのが難しくなる extension が落ちると全 build に影響するので、耐障害性(冗長化)、モニタリングが必要 etc  勿論、上記の懸念点は Extension によって提供される機能とトレードオフであり、 Extension の導入方針は Drone が運用される環境によって大きく依存すると思います。
例えば全員が顔見知りのような小さな組織で特定のサービス専用に Drone を使っていてかつ Drone の運用体制(人員)に十分余裕があるなら 積極的に Extension を導入しても問題ないかもしれません。</description>
    </item>
    
    <item>
      <title>Drone v1 で gRPC が使われなくなった</title>
      <link>https://techblog.szksh.cloud/drone-v1-deprecate-grpc/</link>
      <pubDate>Thu, 15 Aug 2019 07:39:18 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-v1-deprecate-grpc/</guid>
      <description>v0.8 では server - agent 間の通信に gPRC が使われていましたが、 v1 では使われなくなりました。
理由  https://discourse.drone.io/t/curious-about-decision-to-drop-grpc/3987  gRPC関連のトラブルの問い合わせが多すぎてサポートしきれないので止めた    v1 での通信方法  https://discourse.drone.io/t/drone-agents-keep-closing-connections-with-499-code/5197/2  agent がロングポーリングしている 30秒後、なんのビルドもなければコネクションを切って、再接続する(張りっぱなしにしてると、LBやファイアウォールにコネクション切られるため)     自分も v0.8 から Drone を運用していて最近 v1 に upgrade しましたが、 v0.8 では gRPC 関連のトラブルが頻発していました。 server のログでは絶えず gRPC 関連のエラーを吐いていましたし、 server - agent 間の TCP connection が切れっぱなしになって戻らくなって agent 数がどんどん減っていったり ビルドが pending のままになったり、色々ありました。
関連する issue はあり、幾つか対策を打ってみたりしましたが、結局解決しませんでした。
 https://github.com/drone/drone/issues/2090 https://github.com/drone/drone/issues/2246 https://github.com/drone/drone/pull/2294 https://www.reddit.com/r/droneci/comments/8opifu/drone_stops_working_after_some_little_time/e06d1gn/  それが v1 にアップグレードして gRPC が使われなくなってから解消し、個人的にはとても助かりました。 管理者的にはアップグレードして一番嬉しい点ですね。</description>
    </item>
    
    <item>
      <title>Golang での時刻の扱い方を整理する</title>
      <link>https://techblog.szksh.cloud/golang-time/</link>
      <pubDate>Wed, 14 Aug 2019 20:28:56 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/golang-time/</guid>
      <description>今更ながら Golang での時刻の扱い方について改めて整理してみました。
まとめ  time.Local は明示的に設定する(基本UTC) DB などには 基本UTC で永続化する 出力時に必要になったらタイムゾーンを変更する  location は出力時に問題になるので出力時に location を明示的に指定する 逆に言うと出力時以外は問題にならないので無理に location を UTC にしなくても良いかもしれない サードパーティ(ex. ORM) に time.Time を渡す場合は location に注意が必要   文字列として時刻の入力を受け付ける場合は location を明示的にセットする サードパーティが time.Local に依存する場合、 time.Local を明示的に UTC にしたりする必要があるかもしれない アプリケーションで利用する location が分かっている場合、location を取得するヘルパー関数を定義する time.LoadLocation は環境依存なので予め location が分かっているなら使わないほうがよい 文字列を time.Time に変換する場合、time.ParseInLocation で Location を指定して time.Time に変換後、time.Time.UTC() で UTC に変換する time.Time を文字列に変換する場合、time.In で location を変換後、time.Time.Format で文字列に変換する  グローバルな location https://golang.org/pkg/time/#Location</description>
    </item>
    
    <item>
      <title>Drone v1 では Jsonnet が extension なしで使える</title>
      <link>https://techblog.szksh.cloud/drone-jsonnet/</link>
      <pubDate>Fri, 02 Aug 2019 23:02:57 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-jsonnet/</guid>
      <description>Drone では v1 から冗長な YAML を DRY にする一つの手として、 Jsonnet の利用が推奨されています。 これについては過去のブログでも触れています。
https://techblog.szksh.cloud/drone-jsonnet-generator/
しかし、 v1 の rc の時点では Jsonnet の活用には Jsonnet Extension が必要でした。
https://engineering.linecorp.com/ja/blog/go-oss-ci-cd-platform-drone-1-0-0-rc-1/#title7-1
しかし、 v1 の正式版では Jsonnet Extension がなくても Jsonnet が利用できるようになっています。
まず Drone の管理者側で Drone server に環境変数 DRONE_JSONNET_ENABLED=true を設定する必要があります。
そうしたら、ユーザー側は次のようにすることで jsonnet が使えます。
 .drone.yml の代わりに .drone.jsonnet をコミットする (.drone.yml は不要) 各リポジトリの settings の Main &amp;gt; Configuration で設定ファイルのパスを変更する  こうすることでビルド実行時に自動で Jsonnet が YAML に変換され処理されるようです。
いつから Jsonnet Extension は不要になったのか  https://github.com/drone/drone/compare/v1.0.0-rc.6...v1.0.0 https://github.com/drone/drone/commit/5013cfa993fa455fc56f10e45b9f36cf1d6dff57  v1 の rc ではサポートされてませんでしたが、正式版をリリースするタイミングで Jsonnet Extension が不要になっていたようです。</description>
    </item>
    
    <item>
      <title>The Top 10 Most Common Mistakes I’ve Seen in Go Projects を読んでみて</title>
      <link>https://techblog.szksh.cloud/the-top-10-most-common-mistakes-ive-seen-in-go-projects/</link>
      <pubDate>Sun, 21 Jul 2019 18:13:53 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/the-top-10-most-common-mistakes-ive-seen-in-go-projects/</guid>
      <description>The Top 10 Most Common Mistakes I’ve Seen in Go Projects という記事を読んで面白かったのでメモります。 翻訳ではないです。メモなので、原文を読んでください。
 Unknown Enum Value: Unknown であることを表す enum の値は 0 にしよう。値がセットされていない場合に Unknown として扱えるから Benchmarking: ベンチマークを取るのは難しい。コンパイラの最適化によってベンチマークの結果が不適切になる場合がある Pointers! Pointers Everywhere!: パフォーマンスの観点から基本的にはポインタを使うべきではない。変数を共有する必要がある場合のみ、ポインタを使う Breaking a for/switch or a for/select: for, switch が入れ子になっている場合、switch の中で break しても for から抜けられない。抜けたければ labeled break を使う Errors Management Slice Initialization Context Management Not Using the -race Option: go test コマンドでは -race オプションをつけよう Using a Filename as an Input: 引数としてファイル名を渡すのではなく、 io.</description>
    </item>
    
    <item>
      <title>Flute - Golang HTTP client testing framework</title>
      <link>https://techblog.szksh.cloud/fagott/</link>
      <pubDate>Sun, 07 Jul 2019 08:20:00 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/fagott/</guid>
      <description>2019-07-17 追記 プロジェクト名が変わりました
https://github.com/suzuki-shunsuke/flute/issues/20
 Go の HTTP client のテストフレームワークを作ったので紹介します。
https://github.com/suzuki-shunsuke/flute
執筆時点のバージョンは v0.6.0 です。
 リクエストパラメータのテスト HTTP サーバのモッキング  を目的としています。
比較的実践的なサンプルとして、ユーザーを作成する簡単な API client とそのテストを書いたので参考にしてください。
 https://github.com/suzuki-shunsuke/flute/blob/master/examples/create_user.go https://github.com/suzuki-shunsuke/flute/blob/master/examples/create_user_test.go#L17-L53  元々自分はこの目的のために h2non/gock を使っていました。 ただ、 gock だとリクエストがマッチしなかったときに、なぜマッチしないのかがわからず、調査に困るという問題がありました。
そこで flute では request に対し、matcher と tester という概念を導入し、 matcher でマッチしたリクエストを tester でテストするというふうにしました。 テストでは内部で stretchr/testify の assert を使っており、テストに失敗したときになぜ失敗したのかが分かりやすく出力されるようになっています。
例えば以下の例は、リクエストの Authorization header にトークンがセットされていなかった場合のエラーメッセージです。
=== RUN TestClient_CreateUser --- FAIL: TestClient_CreateUser (0.00s) tester.go:168: Error Trace: tester.go:168 tester.go:32 transport.go:25 client.go:250 client.go:174 client.</description>
    </item>
    
    <item>
      <title>Drone で「ビルド実行時にパラメータを渡す」っぽいことをする</title>
      <link>https://techblog.szksh.cloud/how-to-imitate-jenkins-parameterized-build-at-drone/</link>
      <pubDate>Thu, 20 Jun 2019 17:50:11 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/how-to-imitate-jenkins-parameterized-build-at-drone/</guid>
      <description>Jenkins では parameterized build という機能で、ビルド実行時に Web UI からパラメータを指定することができます。
Drone では基本的に Git のイベントをフックして動くので「ビルドを実行時に手動でパラメータを設定する」ということは出来ません。
自分は基本的にできなくても構わないと思っていますが、 こういった機能がないから Drone を使わないという人も中にはいるので、 Drone でもちょっとした工夫でそれっぽいことは出来るんじゃないかと思い、簡単なサンプルを書いてみました。
一応言っておくと、 Jenkins の parameterized build を完全に代替するようなものではありません。
https://github.com/suzuki-shunsuke/example-drone-build-parameter
以下のファイルが必要です。
 build_params/params.sh.tpl: ビルドパラメータを記述するファイルのテンプレート scripts/deploy.sh: デプロイ時に実行するスクリプト .drone.yml: Drone の設定ファイル  スクリプトを実行してデプロイします。
$ bash scripts/deploy.sh するとパラメータを記述するファイルがテンプレートから作成され、エディタで開きます。
https://github.com/suzuki-shunsuke/example-drone-build-parameter/blob/master/scripts/deploy.sh#L12-L17
パラメータを記述し、エディタを閉じます。
するとそのファイルがコミットされ、新しいタグが作成され、コミットとタグがリモートにプッシュされます。
https://github.com/suzuki-shunsuke/example-drone-build-parameter/blob/master/scripts/deploy.sh#L27-L35
Drone でタグをプッシュするイベントをフックしてビルドが実行されます。
https://github.com/suzuki-shunsuke/example-drone-build-parameter/blob/master/.drone.yml#L13-L17
ビルドではコミットされたパラメータの設定ファイルを読み込むことでビルドにパラメータを渡せます。
https://github.com/suzuki-shunsuke/example-drone-build-parameter/blob/master/.drone.yml#L10
こうすることでビルドにパラメータを渡すことができます。 パラメータの設定ファイルはコミットされるので Git で管理できるというのも特徴です。
https://github.com/suzuki-shunsuke/example-drone-build-parameter/blob/master/build_params/2019-07-07T10-04-02JST/params.sh
上記のスクリプトではパラメータの設定ファイルとしてシェルスクリプトで環境変数を定義していますが、 シェルスクリプトである必要性はなく、例えば JSON ファイルを記述してビルドで JSON ファイルを読み込んでもよいし、 パラメータを選択させるようなことがしたければ fzf のようなものを使ってもよいし、 いくらでも改善できます。
以上、簡単な tips でした。</description>
    </item>
    
    <item>
      <title>Drone v0.8 の .drone.yml を v1 の .drone.jsonnet に変換するツールを作った</title>
      <link>https://techblog.szksh.cloud/drone-jsonnet-generator/</link>
      <pubDate>Wed, 12 Jun 2019 07:40:45 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-jsonnet-generator/</guid>
      <description>Drone v0.8 の .drone.yml を v1 の .drone.jsonnet に変換するツールを作ったので紹介します。
https://github.com/suzuki-shunsuke/drone-jsonnet-generator
背景 https://docs.drone.io/user-guide/pipeline/migrating/
Drone は v0.8 から v1 で .drone.yml のフォーマットが大きく変わっています。 Drone v1 ではビルド実行時に自動で変換しているため、v0.8 の .drone.yml でもそのまま動きます(matrix builds も動きます)。
そのため、Drone v0.8 から v1 に移行する際、すぐに .drone.yml を修正しなくても問題ないのですが、 v1 独自の機能が出てきた場合 v0.8 のフォーマットの場合利用できないかもしれませんし、 いつまでも古いままだと気持ち悪いので出来るならフォーマットを変換したいです。
drone-cli ではフォーマットを変換する drone convert というコマンドが提供されています。
ただし、 drone convert は matrix build を multiple pipeline に変換するのですが、 非常に冗長になります。 そのため、jsonnet を利用することが推奨されています。
https://docs.drone.io/user-guide/pipeline/migrating/
 The above syntax can be quite verbose if you are testing a large number of variations.</description>
    </item>
    
    <item>
      <title>.drone.jsonnet と .drone.yml を比較する Drone plugin を作った</title>
      <link>https://techblog.szksh.cloud/drone-plugin-jsonnet-check/</link>
      <pubDate>Sat, 01 Jun 2019 08:34:10 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-plugin-jsonnet-check/</guid>
      <description>久しぶりに Drone plugin を作ったので紹介します。
https://www.github.com/suzuki-shunsuke/drone-plugin-jsonnet-check
.drone.jsonnet から .drone.yml を生成していて、両方を Git で管理している場合に、 .drone.jsonnet と .drone.yml の状態が一致しているかテストするための plugin です。
Drone v1 では matrix builds が廃止され、multiple pipeline が導入されました。 matrix builds を drone convert コマンドで multiple pipeline に変換すると、pipeline の数が多いほど冗長でメンテナンス性が悪くなります。 そこで公式では jsonnet で記述して .drone.yml に変換する方法が推奨されています。
https://docs.drone.io/user-guide/pipeline/migrating/
 To simplify your configuration we recommend using jsonnet.
 $ drone jsonnet --format --stream jsonnet から yaml への変換は Jsonnet extension を使うと Drone がビルド実行時に自動で変換してくれるので .drone.yml を管理する必要はなくなりますが、 使っていない場合、 .drone.jsonnet と .</description>
    </item>
    
    <item>
      <title>go-jsoneq - 2つの値がJSONとして等しいか比較するGoライブラリ</title>
      <link>https://techblog.szksh.cloud/go-jsoneq/</link>
      <pubDate>Thu, 23 May 2019 11:43:18 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/go-jsoneq/</guid>
      <description>https://github.com/suzuki-shunsuke/go-jsoneq
2つの値がJSONとして等しいか比較するGoライブラリを開発したので紹介します。
「2つの値がJSONとして等しい」とは、2つの値をそれぞれJSON文字列に変換したら、2つが表現するデータがおなじになるという意味です。
struct { Foo string `json:&amp;#34;foo&amp;#34;` }{ Foo: &amp;#34;bar&amp;#34;, } と
map[string]interface{}{&amp;#34;foo&amp;#34;: &amp;#34;bar&amp;#34;} を JSON に変換したらともに
{&amp;#34;foo&amp;#34;: &amp;#34;bar&amp;#34;} になりますね。
json.Marshaler のテストや、 実際の JSON 文字列から構造体を定義したときにちゃんと定義できているかチェックするのに使えると思います。
jsoneq.Equal でやっていることは単純です。
 json.Marshal で []byte に変換 json.Unmarshal で []byte を map, array と primitive な型からなるオブジェクト(?)に変換 reflect.DeepEqual で比較  引数が []byte の場合は 1 は飛ばします。
GoDoc やサンプルを見れば使い方は簡単にわかると思います。
以上、簡単ですが、自作ライブラリの紹介でした。</description>
    </item>
    
    <item>
      <title>durl - 壊れたURLを検知するCLIツール</title>
      <link>https://techblog.szksh.cloud/durl/</link>
      <pubDate>Sun, 28 Apr 2019 21:25:00 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/durl/</guid>
      <description>結構前に開発したツールですが、まだ記事にしてなかったので紹介します。
https://github.com/suzuki-shunsuke/durl
ファイル中の URL が壊れていないかチェックするツールです。 ファイル中の URL を抽出し、HTTPリクエストを投げてステータスコードが 2xx でないものがあった場合、異常終了します。
なお、ページ内リンク(アンカー)が壊れているものについては検知できません。
インストール Go製で、バイナリを GitHub Releases で公開しています。
https://github.com/suzuki-shunsuke/durl/releases
Docker イメージ https://quay.io/repository/suzuki_shunsuke/durl
busybox ベースの Docker イメージも提供しています。 CI で使うのに便利です。
使い方 durl init で設定ファイル .durl.yml を生成します。
$ durl init durl check に対象ファイルパスのリストを標準入力として渡してください。 find コマンドなどと組み合わせると良いです。
https://github.com/suzuki-shunsuke/go-errlog/blob/v0.9.0/scripts/durl.sh#L9
find . \ -type d -name node_modules -prune -o \ -type d -name .git -prune -o \ -type d -name vendor -prune -o \ -type f -print | \ grep -v package-lock.</description>
    </item>
    
    <item>
      <title>毎週30分の技術共有会</title>
      <link>https://techblog.szksh.cloud/2019-knowledge-share-meeting/</link>
      <pubDate>Sun, 17 Mar 2019 13:47:40 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/2019-knowledge-share-meeting/</guid>
      <description>自分が最近職場で行っている技術共有の取り組みについて紹介したいと思います。
背景 これまで自分は積極的に自分にとって新しい技術を取り入れてサービスの品質の向上に繋げてきました。 ただし、それらの技術に関して周りに十分に共有できていなかった側面がありました。
やっていること 毎週30分決まった時間にスライドを使って発表しています。 対象は同じ部署の希望者です。 枠は30分ですが、実質話しているのは20分くらいな気がします。 k8sのハンズオン的なこともやりました(そのときは30分で終わらないので2回に分けてやりました)。
話したいことはたくさんあるのですが、とりあえず大きなトピックとして以下の3つに絞っています。
 k8s(Rancher): オーケストレーション (いまここ) Drone: CI/CD Graylog: ログ収集  これまで話したこと・話す予定のこと k8s の初心者が k8s を本番運用を視野に入れつつ検証環境で使ってみるところまでを目指して話しています。
 なぜ k8s を使うのか(部署のコンテキストに合わせて導入意義を説明) k8s のリソース(Pod, Service, Deployment, etc) について k8s, Rancher ハンズオン(2回) 簡単なアプリケーションをデプロイしてみたり Logging (いまここ) モニタリング IP制限のかかった外部サービスへアクセスする方法  毎週30分というペース感について 以下のようなことを配慮しました。
 集中力が続くこと  60分は長すぎる   持続可能であること  1, 2 回やっただけでは意味がない 30分だけなら参加しやすい 準備のコストも現実的な範囲 30分と短めなので毎週やる。隔週とかだと頻度が少なすぎるし、1回飛ぶと1ヶ月空いてしまう    これまでの結果 特に大きな成果があるわけではないですが、 k8sに興味を持ちk8sを検証環境で使ってくれる人が出てきました。 共有会がk8s を触るきっかけになったのだとしたらそれだけでもやってよかったと思います。
また、自分自身学ぶこともありました。 Logging に関して自分は今まで Sidecar pattern を使っていたのですが、Cluster Level Logging への移行を検討するきっかけになりました。</description>
    </item>
    
    <item>
      <title>Rancherでusername が重複してログインできなくなった場合の解消方法</title>
      <link>https://techblog.szksh.cloud/rancher-duplicated-user-name/</link>
      <pubDate>Sat, 16 Mar 2019 21:17:05 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/rancher-duplicated-user-name/</guid>
      <description>先日起こった Rancher のトラブルの解消方法について紹介したいと思います。 Rancher のバージョンは v2.1.6 です。 admin ユーザーでログインしようとしたところ、エラーが起こりました。 最初パスワードが間違っているのかと思い、パスワードリセットしたものの、解消しませんでした。
https://rancher.com/docs/rancher/v2.x/en/faq/technical/#how-can-i-reset-the-admin-password
エラーメッセージをよく見ると 500 エラーでした。そこで rancher のコンテナのログを見ました。
[ERROR] API error response 500 for POST /v3-public/localProviders/local?action=login. Cause: found more than one users with username admin username が admin のユーザーが複数人いるからログインに失敗しているようです。 であれば、ユーザーを rename ないし delete すれば解消しそうです。 しかし Admin 権限を持っているのが admin しかいないため、ユーザーを rename したり delete するのが難しいです。
どうすればよいかと思って調べてたところ rancher のコンテナ内で kubectl コマンドを使うことで Rancher の Custom Resource を操作できそうなことを知りました。
https://qiita.com/yamamoto-febc/items/498b911611dd25351ad7
そこで 2 人いる admin の片方を rename することで解消しました。
# rancher のコンテナに入る $ docker exec -ti rancher bash # CRDの一覧 $ kubectl get crd # ユーザー一覧 $ kubectl get users.</description>
    </item>
    
    <item>
      <title>Goの設定管理で viper の代わりに confita を使う</title>
      <link>https://techblog.szksh.cloud/use-confita/</link>
      <pubDate>Sat, 16 Feb 2019 18:38:45 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/use-confita/</guid>
      <description>Golang の設定管理のライブラリといえば viper が有名ですが、 confita も良さそうだったので紹介したいと思います。
confita の機能としては以下のようなものがあります。
 構造体に設定をマッピング flag や環境変数、設定ファイルに対応 複数の設定ファイルに対応  構造体に設定をマッピングすることで、https://github.com/go-playground/validator のようなライブラリを使って設定のバリデーションが出来ます。
また viper は v1.3.1 の時点で複数の設定ファイルを扱いにくいです。
 Viper can search multiple paths, but currently a single Viper instance only supports a single configuration file.
 k8s で ConfigMap と Secret を設定ファイルとして扱う場合、複数のファイルを扱えないと不便です。 その点 confita は複数の設定ファイルを問題なく扱えます。
以下はフラグで指定した複数の設定ファイルから設定を読み込む簡単なサンプルです。
import ( &amp;#34;context&amp;#34; &amp;#34;gopkg.in/go-playground/validator.v9&amp;#34; &amp;#34;github.com/heetch/confita&amp;#34; &amp;#34;github.com/heetch/confita/backend&amp;#34; &amp;#34;github.com/heetch/confita/backend/file&amp;#34; flag &amp;#34;github.com/spf13/pflag&amp;#34; ) func loadConfig(ctx context.Context) (Config, error) { cps := flag.StringSliceP(&amp;#34;config&amp;#34;, &amp;#34;c&amp;#34;, nil, &amp;#34;configuration file path&amp;#34;) flag.</description>
    </item>
    
    <item>
      <title>JS以外でのnpmの活用</title>
      <link>https://techblog.szksh.cloud/use-npm/</link>
      <pubDate>Thu, 14 Feb 2019 21:34:22 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/use-npm/</guid>
      <description>npm は Node.js のパッケージマネージャーですが、自分はJS以外のプロジェクトでも使えると思っています。 実際、Goのアプリケーション、OSS、ansible role, playbook など種類を問わず、自分が管理している多くのリポジトリで使っています。 ただ、GoのOSSで npm 使っているのは自分以外で見たことはないですし、 正直あまり賛同はされないかなと思いますが、こういう考え方もあると思っていただけたらと思います。
npm を使う理由は
 Node製のツールを使うため npm scripts を使うため (今回書きたいのはこっち)  の2つあります。
Node製のツール  husky: Git Hookを設定 commitlint: commit メッセージのlint standard-version: コミットログによって Change Log を生成  などを使っています。 Nodeはバージョンの変化が速く、互換性が壊れたりとかも多い印象ですが、 グローバルにインストールしなくてもリポジトリごとに install 出来る(package.jsonで管理できる)のでその点は(特にチーム開発では)良いと思います。
npm scripts npm scripts によってそのリポジトリの開発に使うコマンド群を管理するということを自分はしています。
https://github.com/suzuki-shunsuke/gomic/blob/v0.5.7/package.json
なにもツールを使わない場合に比べ、こうすることでチーム全体でコマンドを統一できますし、一連のコマンドをスクリプト化して npm scripts で実行できるようにするなど、自動化も促進されます。
ごく簡単な自動化の例ですが、tag を打つと同時にソースコード中のバージョン番号を更新するのを npm run tag v1.1.0 といったコマンドで出来るようにしています。 こうすることで tag とversionコマンドで出力されるバージョンが違うなんてことを防ぐことが出来ます。
https://github.com/suzuki-shunsuke/gomic/blob/v0.5.7/scripts/tag.sh
また、オプションによって動作が変わるようなコマンドは npm scripts によって実行することでオプションを統一できます。 例えば gofmt は -s オプションの有無で結果が変わります。</description>
    </item>
    
    <item>
      <title>Golang の好きなところ</title>
      <link>https://techblog.szksh.cloud/golang-good-point/</link>
      <pubDate>Sun, 10 Feb 2019 17:49:58 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/golang-good-point/</guid>
      <description>自分は 2017/8頃(曖昧)からメインで書く言語をPythonからGolangに変更しました。 Goを書き始めて割と早い段階でGoが一番好きになりました。 そこでなんで Go が好きなのかということを頑張って言語化しようと思います。
若干他の言語と比較する部分もありますが、決して他の言語をディスったり、 他の言語より優れているということが言いたいわけではないのでご了承ください。
 依存するものが小さく、バイナリ1つインストールするだけで良い  Prometheus の exporter とかインストールするの簡単 Docker Imageも最小限になる   静的型付け  ビルド出来ている時点で一定の信頼性が担保されている よく知らないコードを読んだり修正するときとかだいぶ有り難い   GoDocが素晴らしい  何もしなくてもライブラリのドキュメントが出来上がっている   ライブラリの公開が容易  GitHubに公開するだけ npm や pypi のようなレジストリがないので楽   go test とか go vet, gofmt みたいに標準ツールが揃っている コーディング規約で悩む必要がない lintツールが充実している  gometalinter とか使っておけば OK lintできる環境を構築するのにそこまで頑張らなくて良い   エラーハンドリングが暗黙的に省略できないので信頼性が高い  Goのエラーハンドリング嫌いって人もいるし、v2で改善されるって話も聞くけど、自分はむしろ好き(面倒なのは理解できるけど)   言語仕様がシンプル(客観的な根拠はないし、難しい部分もあるけど、そんな気がする)  メタプログラミング使った、魔術的なコードになりにくい   interface 使ってコードを疎結合にするのが書いてて気持ちいい 並列処理が書きやすい  </description>
    </item>
    
    <item>
      <title>go-error-handling-logging-practice v0.2</title>
      <link>https://techblog.szksh.cloud/golang-logging-error-handling-practice-0.2.0/</link>
      <pubDate>Fri, 01 Feb 2019 22:26:13 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/golang-logging-error-handling-practice-0.2.0/</guid>
      <description>以前 Golang のロギング・エラーハンドリングについて書きました。
 https://suzuki-shunsuke.github.io/golang-logging-error-handling-practice/ https://github.com/suzuki-shunsuke/go-error-handling-logging-practice  それを少し v0.1 から v0.2 に互換性を壊す形でアップデートしようかと思います。 本記事ではその変更点について書きます。
変更点 関数のエラーに情報を付与する責務を関数に割り当てていたものを、呼び出し元に割り当てるようにします。
具体的には元々
func createUser(name string, age int) error { return errlog.Wrap(checkName(name), logrus.Fields{&amp;#34;age&amp;#34;: age}, &amp;#34;failed to create a user&amp;#34;) } だったものが
func createUser(name string, age int) error { return errlog.Wrap(checkName(name), nil, &amp;#34;user name is invalid&amp;#34;) } になります。
変更理由 メタ情報のフィールド名はコンテキストに依存します。 上記の例だとユーザー名というメタ情報のフィールド名は name より user_name や admin_name, owner_name としたほうが適切かもしれません。それは関数内部では分からず、呼び出し元でないと分かりません。呼び出し元でないとフィールド名の衝突が避けられないこともあるでしょう。
メッセージに関しても同様のことが言えます。 また、元々 v0.1 ではユーザーが定義した関数と
 標準関数やサードパーティのライブラリなど、プロジェクト外部で定義された関数 interface の関数やメソッド  を区別し、前者では関数側でエラーに情報を付与させる一方、後者では呼び出し元で情報を付与させるというふうにしていました。</description>
    </item>
    
    <item>
      <title>Terraform Providerで import を実装する方法</title>
      <link>https://techblog.szksh.cloud/terraform-provider-graylog-import/</link>
      <pubDate>Fri, 25 Jan 2019 22:38:28 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/terraform-provider-graylog-import/</guid>
      <description>terraform provider graylog で alert condition と stream rule の import を実装しました。
 https://github.com/suzuki-shunsuke/go-graylog/pull/59 https://github.com/suzuki-shunsuke/go-graylog/pull/60  そこで import を実装する方法を紹介したいと思います。
terraform でリソースをimportするにはリソースがimportをサポートしている必要があります。 schema.Resource の Importer フィールドですね。リソースがIDだけでGet出来る場合、schema.ImportStatePassthroughを使えば終わりです。 一方、Graylogのalert condition や stream rule はIDだけでなく、stream id も必要になります。 terraform import コマンドは1つの引数しか取らないため、サポートできないのでは？と以前まで思っていました。 そういった場合、次のようにStateFuncを実装すればサポートできます。
https://github.com/suzuki-shunsuke/go-graylog/pull/59/commits/baee1165f49d2bc21b6ea7551ceff6b7daf01543#diff-f41be2a3640efd12ad4e808d77c5c8d5
# &amp;quot;/&amp;quot; で区切って stream id と ID を渡す $ terraform import graylog_alarm_callback.test 5bb1b4b5c9e77bbbbbbbbbbb/5c4acaefc9e77bbbbbbbbbbb 区切り文字は何でも良いのでしょうが、公式のprovider が &amp;ldquo;/&amp;rdquo; で区切っていたのでそれに従うことにしました。
https://www.terraform.io/docs/providers/google/r/spanner_database.html#import
https://godoc.org/github.com/hashicorp/terraform/helper/schema#ImportStatePassthrough の実装を見てみれば分かりますが、 StateFunc の中では GET API を叩いてリソースを取得したりはしません。 terraform import コマンドの標準出力を見ると分かりますが refresh を実行しているのでそこでGETしているようです。 StateFunc は *schema.</description>
    </item>
    
    <item>
      <title>GithubをFree Planにダウングレードした</title>
      <link>https://techblog.szksh.cloud/downgrade-github-plan/</link>
      <pubDate>Sun, 20 Jan 2019 12:27:26 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/downgrade-github-plan/</guid>
      <description>GitHub のプラン体系が変わり、無料プランでも無制限でprivate repositoryが作れるようになりました。
https://github.blog/2019-01-07-new-year-new-github/
そこで無料プランにダウングレードすることにしました。
 https://help.github.com/articles/downgrading-your-github-billing-plan/ https://blog.jnito.com/entry/2019/01/09/081913  無料プランではwikiはpublic repositoryでしか使えないので、 private repository の wiki を 移行することにしました。
private なソースコード(サービス)のためのwikiではなく、 個人的なメモが書いてあるだけだったので移行することに特に問題はありませんでした。
全 private repository の wiki を clone そこでまずはそういった wiki を clone して一つのリポジトリにまとめることにしました。
https://github.com/suzuki-shunsuke/foo の wiki は https://github.com/suzuki-shunsuke/foo.wiki で clone できます。
次のようなコマンドを実行し、private repositoryのwikiを全部cloneしました。
https://developer.github.com/v3/repos/#list-your-repositories
curl &amp;#34;https://api.github.com/user/repos?access_token=$GITHUB_TOKEN&amp;amp;visibility=private&amp;#34; | jq -r &amp;#39;.[].html_url&amp;#39; | xargs -I{} -n 1 git clone {}.wiki wikiが存在しないものに関しては clone に失敗します。 API で wiki のリストが取得できると良かったんですが、 wikiに関するAPIはなさそうです。
また /user/repos API のレスポンスの has_wiki はwikiが存在しなくても、wikiが無効化されてなければ true なようです。</description>
    </item>
    
    <item>
      <title>Golangにおけるエラーハンドリングとロギングのプラクティス</title>
      <link>https://techblog.szksh.cloud/golang-logging-error-handling-practice/</link>
      <pubDate>Tue, 25 Dec 2018 21:51:41 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/golang-logging-error-handling-practice/</guid>
      <description>2018-12-30 追記 この記事を元にドキュメントを書いてみました。
https://github.com/suzuki-shunsuke/go-error-handling-logging-practice
追記ここまで
 Golang でエラーハンドリングとロギングをしてきて自分の中で固まりつつあるプラクティスを明文化します。 明文化することで以下のことを目指します。
 迷いをなくす コードの一貫性を保つ コーディング規約とすることでレビューの品質を上げる(自動化は出来ないけど) コードの品質を上げる(コードがゴチャつかなくなる) 適切にエラーをロギングする(必要十分な情報をログとして残す)  またエラーハンドリングとロギングのためのライブラリを自作しているのでそれも紹介します。
https://github.com/suzuki-shunsuke/go-errlog
ロギングに関する関連記事 この記事を書く前に軽くググってみただけでちゃんと読んでないのですが、 興味のある人は読んでみてください。
 https://www.loggly.com/blog/think-differently-about-what-to-log-in-go-best-practices-examined/ https://dave.cheney.net/2015/11/05/lets-talk-about-logging https://postd.cc/go-best-practices-2016/#logging-and-instrumentation  ログレベルは分ける ログレベルでwarningとかいらないという意見もありますが、自分は必要だと思っています。 自分は以下のログレベルを使い分けます。
 debug: あまり使わない。調査目的で一時的に埋め込むログ。調査が終わったら出力しないようにする。一時的でないものはinfoにする info: エラーでないログ。イベント、処理の開始時や終了を記録するのに使うことが多い warn: 4xx系のエラー。それが起こっただけではアラートを飛ばさないが、数が通常時より多い場合はバグかUIに問題があってユーザーが間違えやすくなっている可能性があるのでアラートを飛ばす error: 5xx系のエラー。アラートを飛ばす(閾値は調整) fatal: 処理継続が不可能な致命的なエラー。システムを止める  書いてから思いましたが、これに関しては標準的な使い分けのルールがありそうですね(要調査)。。
logrus を使ってログを構造化する 前提としてwebシステムやバッチシステムなどを想定しています。CLIツールならば話は変わるでしょう。 JSONフォーマットで出力してfluentdでElasticsearchにフォワードするのが個人的によくあるパターンです。
go-errlogもlogrusの使用を前提としています。
ロギングのライブラリは他にも色々あるので、logrusで満足できない人は以下から探してみるとよいでしょう。
https://github.com/avelino/awesome-go#logging
エラーログは中央集権的に main に近い所で出力する エラーログをどこで出力するかですが、原則中央集権的に main に近い所で出力します。 因みに中央集権的という表現は echo の centralized error handling からもじっています。
https://echo.labstack.com/guide/error-handling
error が発生してもすぐログを吐くのではなく、error を関数の戻り値として返し、ロギングする責務を親に委譲します。 Goでは以下のようなイディオムがよく見られますね。
if err != nil { return err } ロギングに必要な情報を戻り値のerrorに含める 上記のコードで問題なのは、エラーに関する情報が欠損することがあることです。</description>
    </item>
    
    <item>
      <title>GraylogのAlertの課題</title>
      <link>https://techblog.szksh.cloud/graylog-alert-issue/</link>
      <pubDate>Wed, 19 Dec 2018 21:02:11 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/graylog-alert-issue/</guid>
      <description>Graylogを運用してきて感じているAlert機能周りの課題をリストアップします。 自分のGraylogの理解が不十分で勘違いしている部分もあるかもしれませんが、ご了承ください。 Graylogのバージョンは 2.5.0 です。 ここでいう「メンション」とは、Slackのようなチャットツールのメンションを指します。 リストの詳細を書きだしてみたものの、リストだけでだいたい言いたいことが言えてしまっていたのと、単なる愚痴っぽくなってしまったので、 リストだけに留めます。
2018-12-31 追記 元々 Alert Condition, Notification の APIがないと勘違いしていたのですが、 実はちゃんとあったので terraform で管理できるように go-graylog を更新しました。
 https://github.com/suzuki-shunsuke/go-graylog/pull/50 https://github.com/suzuki-shunsuke/go-graylog/pull/52 https://github.com/suzuki-shunsuke/go-graylog/blob/v11.0.0/docs/resources/alarm_callback.md https://github.com/suzuki-shunsuke/go-graylog/blob/v11.0.0/docs/resources/alert_condition.md  課題リスト  APIでAlert Condition, Notificationを管理できない  APIがないので terraform でサポートも出来ない 数が増えるとWeb UIでは管理が辛い・修正漏れや設定ミスが出やすい   Condition, NotificationがStreamに紐づく  ConditionによってNotificationを変えられない  条件に応じてアラートの文面・通知先・メンション先・メンションの有無を変えられない (正確にはテンプレートエンジンで頑張ればある程度対応できるかもしれないが、個人的にはテンプレートそのものを切り替えたい)   ConditionやNotificationを複数のStreamで使い回せない   (少なくとも標準機能では)時間帯によってアラートの挙動を変更できない  夜中にはアラートを飛ばさない・メンションをつけないといったことが出来ない 一時的にアラートを止められない   Pluginを使うにしてもどれを使ったら良いか分からない  もっとGraylogがメジャーになれば状況も変わるかもしれない    </description>
    </item>
    
    <item>
      <title>Graylog の Terraform を CI/CDで実行する</title>
      <link>https://techblog.szksh.cloud/graylog-terraform-ci/</link>
      <pubDate>Fri, 07 Dec 2018 08:22:49 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/graylog-terraform-ci/</guid>
      <description>以前 Graylog を Terraform で管理する記事を書きました。
https://suzuki-shunsuke.github.io/graylog-terraform/
今回はそれを CI/CD で実行できるようにした話です。
ただし、今回の内容は Graylog に限らず Terraform を CI/CD で実行する方法として使えると思います。
今回実現したのは以下のことです。
 PR時にテストをする plan/* tag を push すると terraform plan が実行される apply/* tag を push すると terraform apply が実行され、tfstate がコミット、プッシュされる  ソースコード https://github.com/suzuki-shunsuke/example/tree/master/graylog-terraform に置いておきました。
 https://github.com/suzuki-shunsuke/example/blob/master/graylog-terraform/role.tf#L13-L25 https://github.com/suzuki-shunsuke/example/blob/master/graylog-terraform/user.tf#L12-L21 https://github.com/suzuki-shunsuke/example/blob/master/graylog-terraform/.drone.yml https://github.com/suzuki-shunsuke/example/blob/master/graylog-terraform/terraform.tfvars.tpl https://github.com/suzuki-shunsuke/example/blob/master/graylog-terraform/drone_pipeline_commands/git.sh  CI/CD用の user, role を作成する まずは role を作成します。
resource &amp;quot;graylog_role&amp;quot; &amp;quot;terraform&amp;quot; { name = &amp;quot;terraform&amp;quot; description = &amp;quot;terraform&amp;quot; permissions = [ &amp;quot;dashboards:*&amp;quot;, &amp;quot;indexsets:*&amp;quot;, &amp;quot;inputs:*&amp;quot;, &amp;quot;roles:*&amp;quot;, &amp;quot;streams:*&amp;quot;, &amp;quot;users:*&amp;quot;, ] } permission は terraform で管理するリソースのみ付与しますが、 それでも結構強い権限を付与するので取扱に注意してください。</description>
    </item>
    
    <item>
      <title>molecule でansible の role と playbook をテストする</title>
      <link>https://techblog.szksh.cloud/ansible-molecule/</link>
      <pubDate>Thu, 06 Dec 2018 23:08:04 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/ansible-molecule/</guid>
      <description>ansible 専用の testing ツール molecule を紹介します。 molecule の公式ドキュメント以外の情報は少ないので、参考になれば幸いです。
以前 Docker を使って ansible のテストをする方法を紹介しました。
https://suzuki-shunsuke.github.io/test-ansible-on-docker/
この際は Docker Compose と簡単なシェルスクリプトを使って実現しました。 これはこれでブラックボックスな部分がなく、学習コストも低くて悪くないので興味のある方はそちらもご参照ください。
molecule は ansible 専用の testing ツールです。 基本的に playbook というより role 用のツールですが、playbookのテストも工夫すれば出来ます。
 情報が少ない 公式ドキュメントも分かりづらい部分がある コマンドがエラー吐いて失敗した際に、ググっても情報が出てこないので辛い  という風に辛い部分もありますが、
 star数はそれなりにある ansible の公式のプロジェクトである https://github.com/ansible/molecule/ geerlingguy さんも使ってる  という風に良い面もあります。
それでは使っていきましょう。
インストール https://molecule.readthedocs.io/en/latest/installation.html
$ pip install molecule Docker を使う場合 docker-py も必要です。
$ pip install docker-py role のテスト playbookに比べて role のテストは簡単です。
role のディレクトリ(tasksやfilesなどがあるディレクトリ)に移動してコマンドを実行します。
$ molecule init scenario -r &amp;lt;role name&amp;gt; こうすると molecule ディレクトリが生成されます。</description>
    </item>
    
    <item>
      <title>GraylogをTerraformで管理する</title>
      <link>https://techblog.szksh.cloud/graylog-terraform/</link>
      <pubDate>Sat, 01 Dec 2018 14:56:00 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/graylog-terraform/</guid>
      <description>Graylogのリソースを terraform で管理するために作った terraform provider を紹介します。 Graylogとは何かはこちらを読んでください。
Graylogには様々なリソースがあります。
 User Role Input Index Set Stream Stream Rule Dashboard Alert etc  これらのリソースはWeb UIから作成したり出来るわけですが、 Web UIでポチポチするのは疲れますし、ソースコードで管理したいものです(Infrastructure as Code)。 また、Web UIからでは細かな権限管理は出来ず(限られた権限管理しか出来ない)、APIを使ってする必要があります。
APIを使って管理できるツールを探したものの見つからなかったので、 APIを使ってGraylog用のterraform providerを自作しています。
https://github.com/suzuki-shunsuke/go-graylog
GraylogのAPIの種類は非常に多く、残念ながらカバーできているのは一部だけですが、以下のようなものをサポートしています。
 Alert Condition Alert Notification (Alarm Callback) Input User Role Index Set Stream Stream Rule Dashboard Ldap Setting  Role はサポートしているので権限管理は問題なく出来ます。 Dashboard Widget もサポートしたいです。
出来れば Alert の設定も出来ると良いのですが、Alertに関するCRUD APIが提供されていない(GETのみ)ので、サポートできません。
terraform を使った管理方法 以下では自分の管理方法を紹介します。
https://github.com/suzuki-shunsuke/example/tree/master/graylog-terraform
にサンプルが置いてあります。
基本はプロジェクトごとに
 Index Set, Stream, Role といったリソースを作成 User に Role を付与  という流れになります。</description>
    </item>
    
    <item>
      <title>Graylog で log を管理する</title>
      <link>https://techblog.szksh.cloud/graylog/</link>
      <pubDate>Tue, 27 Nov 2018 16:40:33 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/graylog/</guid>
      <description>Java 製の OSS ログ管理システム Graylog の紹介です。 Graylog については幾つかに分けて記事を書きたいと思います。 今回はGraylogの入門的な内容になります。
なお、本記事中で「現在」「現時点」といった場合、特に断りがなければ記事執筆時点 2018-11-27 を指します。
Graylog のバージョン 検証に用いるGraylogのバージョンは 2.4.6 になります。
OSSバージョンとEnterpriseバージョンがありますが、本記事ではOSSバージョンを使用します。
Graylog とは  https://www.graylog.org/ https://github.com/Graylog2/graylog2-server  Kibana と Elasticsearch(以下ES) を使ったことがある人は、Kibanaに代わるものだと思っていただくとイメージしやすいかと思います。 ログはGraylogそのものが保持するのではなく、ESにインデキシングされます。 Kibana同様、ESに収集されたログを検索したり、ダッシュボードを作ったり出来ます。 ダッシュボードに関してはKibanaのほうが優れているようにも思えますが、 Graylogは認証・認可によりダッシュボードやログを操作できる人を制限・管理することが出来ます。
Graylogでログを管理する場合、ユーザーは直接ESにはログを送らず、Graylogを経由して送ります。 ESに対するGraylog以外のアクセスを制限し直接ESにアクセスされるのを防ぐことが出来ます。
Graylog は多機能なシステムであり、ログを整形したり、アラートを飛ばしたり、他のシステムにログをフォワードしたりすることも出来ます。 marketplace でサードパーティの plugin が公開されており、機能を拡張することが出来ます。 APIも提供されており、ある程度自動化が可能です。
認証・認可 オンプレミスでログを管理する場合、社外からは勿論社内からのアクセスも制限したいです。 Graylog では LDAP や Active Directory によってアクセスを制限できます。 リソース毎に誰が何を出来るか設定できます。
http://docs.graylog.org/en/2.5/pages/users_and_roles/external_auth.html
ログの収集 ログの収集をするには Graylog で幾つかのリソースを作成する必要があります。
 Input Index Set Stream Stream Rule  Input はログの入力のフォーマットの設定であり、 どのポートでどういったフォーマットのログを受け付けるかという設定になります。 フォーマットは様々なものがサポートされています。
 AWS Flow Logs AWS Cloud Watch Logs AWS Cloud Trail Beats CEF AMQP CEF Kafka CEF TCP CEF UDP Fake HTTP Message GELF AMQP GELF HTTP GELF Kafka GELF TCP GELF UDP JSON Path NetFlow UDP Raw AMQP Syslog AMQP Syslog Kafka Syslog TCP Syslog UDP  この設定はログを収集するアプリケーションごとに設定するというより、グローバルな設定なので、他のアプリケーションで既に同じ形式でログを収集していたら新たに設定する必要はありません。</description>
    </item>
    
    <item>
      <title>akoi - binary installer</title>
      <link>https://techblog.szksh.cloud/akoi/</link>
      <pubDate>Wed, 31 Oct 2018 08:56:04 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/akoi/</guid>
      <description>自作のOSS akoi の紹介をします。
 なぜこんなものを作ったのか akoi と ansible を使ってサーバにバイナリをインストールする方法  について主に説明します。
まとめ  akoi はバイナリファイルのインストーラ 設定ファイルで管理できる 冪等であり、効率よくインストールできる  並列インストール Accept-Ranges による分散ダウンロード   ansibleでサーバにバイナリをインストールするのを補助してくれる  ansible で真面目にバージョンコントロールして効率よくインストールするのは難しい(ほとんどの ansible role は出来ていない)    akoi とは akoi はバイナリファイルのインストーラです。 設定ファイルにインストールするファイルのダウンロードURLとインストール先を記述して管理します。 インストールするバイナリのバージョン管理が可能であり、既にインストールしてあるバージョンへの切り替えはシンボリックを作り直すだけなので一瞬で終わります。無駄にダウンロードをしたりはしません。 複数のバイナリを並列でインストールしたり、Accept-Ranges ヘッダによる分散ダウンロードをサポートしています。
分散ダウンロードについては
https://qiita.com/codehex/items/d0a500ac387d39a34401
が参考になります。
Goで書かれています。
https://github.com/suzuki-shunsuke/akoi/releases からバイナリをダウンロードしてインストールできます。
詳細はREADMEを読んでください。
なぜ作ったのか サーバにバイナリをインストールする ansible role を書くのが辛かったからです。 最近は色々なソフトウェアがGoで書かれ、バイナリで配布されています。 そういったバイナリをサーバへインストールするのは ansible で行っているという方も少なくないのではないでしょうか？ 有名なソフトウェアをインストールする ansible role は大抵Ansible Galaxy で公開されています。
しかし、ほとんどの role は「真面目に」バージョン管理していません。 ここでいう「真面目に」とは
 バージョンを指定できる バージョンを変更できる 指定したバージョンが既にインストールされている場合は無駄にダウンロードしたりしない  といったことです。</description>
    </item>
    
    <item>
      <title>gomic - Goのモックジェネレータ</title>
      <link>https://techblog.szksh.cloud/gomic/</link>
      <pubDate>Tue, 30 Oct 2018 08:35:16 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/gomic/</guid>
      <description>自作のOSS gomic の紹介をします。
 なぜわざわざこんなものを作ったのか 生成されたモックの簡単な使い方  を主に説明したいと思います。
まとめ  gomic は Goのinterfaceを実装したモックを生成するCLIツール モックを手で書くのが辛すぎた &amp;amp; 既存ツールで満足できなかったため作った  自動生成できるコードは自動生成すべき   設定ファイルで管理するため、interfaceの更新に合わせてmockの更新が容易 生成されるモックはシンプルなAPIのみ提供するので学習コストが低い  gomic とは gomic は Goのinterfaceを実装したモックを生成するCLIツールです。 これによってモックを使ったテストの作成を効率化します。 単調な作業を自動化し、本来注力すべきことに注力できるようにするためのツールです。
Goで書かれています。
https://github.com/suzuki-shunsuke/gomic/releases からバイナリをダウンロードしてインストールできます。
同様のツールは幾つかあります。
 https://github.com/avelino/awesome-go#testing https://github.com/golang/mock (以下 gomock) https://github.com/gojuno/minimock (以下 minimock)  特に gomock は有名ですね。
なぜ作ったのか 上述のように既に同様のツールはありますし、 gomock と minimock は試しました。 しかしあまり満足のいくものではなかったため、自分で作ることにしました。
自分が欲しかったのは学習コストの低いシンプルなAPIです。 interfaceのメソッドを実装した関数をモックに渡すことで 簡単にメソッドの実装を切り替えたいのです。
// Getwd メソッドのモック mock.SetFuncGetwd(func() (string, error) { return &amp;#34;/tmp&amp;#34;, nil }) mock.Getwd() // &amp;#34;/tmp&amp;#34;, nil これは非常にシンプルで分かりやすく、柔軟性のあるパターンです(minimockはこのパターンもサポートしています)。</description>
    </item>
    
    <item>
      <title>Dockerを使ってansible playbookをテストする</title>
      <link>https://techblog.szksh.cloud/test-ansible-on-docker/</link>
      <pubDate>Fri, 12 Oct 2018 21:02:51 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/test-ansible-on-docker/</guid>
      <description>ansible playbookを(VMの代わりに)Dockerコンテナに対して実行すると、効率よく動作確認できるというお話です。 ansible playbookの動作確認のためにvagrantでVMを起動したりしていると動作確認に時間がかかるし、容量も喰います。 Dockerコンテナを使って動作確認できればこれらの問題を解決できます。
ただし、Dockerコンテナの場合、一部のansible のtaskが失敗することがあるので ansible playbookによってはDockerコンテナではテストにならない場合もあるかと思います。
サンプル https://github.com/suzuki-shunsuke/example/tree/master/ansible/test-on-docker
簡単なサンプルを用意しました。
ansible playbookに加え、
 docker-compose.yml test_docker.sh: 簡単なシェルスクリプト  を作成してあります。
$ bash test_docker.sh とすることで動作確認できます。
説明 スクリプトでやっていることは
 Docker Compose でコンテナを起動 コンテナのIDを取得し、inventory ファイルを作成 Docker Connection Pluginを使ってコンテナにplaybookを実行  です。
Docker Connection Plugin を使うとDockerコンテナに対してansible playbookを実行できます。
 https://docs.ansible.com/ansible/2.6/plugins/connection.html https://docs.ansible.com/ansible/2.6/plugins/connection/docker.html  また、Docker Hubで公開されている多くのDockerイメージのUSERは root ですが、 ansible playbookの動作確認としては都合が悪かったりします。
そこで自分は非rootユーザーを作成したDocker Imageを使っています。
https://hub.docker.com/r/suzukishunsuke/ansible-test-centos/
関係ありそうなツール 今回は簡単なスクリプトとdocker-compose.ymlを用意しましたが、 もっとイケてるやり方がありそうなものです。
Vagrant Docker Provider https://www.vagrantup.com/docs/docker/
なんかあまり使えなさそうです。 Linux以外だと結局VMが必要で、既に非推奨な boot2docker をデフォルトでは使うらしくだめそうだなって思いました。 あまり調べてません。
https://www.vagrantup.com/docs/docker/basics.html#host-vm
 By default, the host VM Vagrant spins up is backed by boot2docker, because it launches quickly and uses little resources.</description>
    </item>
    
    <item>
      <title>Project site を User site に移行しました (GitHub Pages)</title>
      <link>https://techblog.szksh.cloud/migrate-to-user-gh-pages/</link>
      <pubDate>Wed, 10 Oct 2018 21:14:03 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/migrate-to-user-gh-pages/</guid>
      <description>なんとなく試験的に Project site でホスティングしていた GitHub Pages を User site に移行しました。
source code は source branch にあります。
CIに関しては Hugo製ブログをGitHub PagesへCIでデプロイ にも書きましたが参考になる部分もあるかもしれません。</description>
    </item>
    
    <item>
      <title>Hugo製ブログをGitHub PagesへCIでデプロイ</title>
      <link>https://techblog.szksh.cloud/how-to-host-hugo-at-github-pages/</link>
      <pubDate>Mon, 01 Oct 2018 08:04:16 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/how-to-host-hugo-at-github-pages/</guid>
      <description>https://gohugo.io/ と https://pages.github.com/ の話です。 master に push したら GitHub Pages に circle ciでデプロイするようにする方法の紹介です。
前提  テーマは master branch に含めない(CIでcloneする) buildしたものは gh-pages ブランチにデプロイ  コード .circleci/config.yml
--- version: 2 jobs: build: docker: - image: suzukishunsuke/hugo-ci:0.1.2 steps: - checkout - run: git config user.name &amp;#34;***&amp;#34; - run: git config user.email &amp;#34;***@example.com&amp;#34; # --depth 1 で高速化 - run: git clone --depth 1 https://github.com/suzuki-shunsuke/tale-hugo themes/tale - run: hugo - run: sh release.sh workflows: version: 2 build: jobs: - build: filters: branches: only: master release.</description>
    </item>
    
    <item>
      <title>travis ci から circle ci への移行のすすめ</title>
      <link>https://techblog.szksh.cloud/migrate-from-travis-ci-to-circle-ci/</link>
      <pubDate>Mon, 01 Oct 2018 07:23:15 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/migrate-from-travis-ci-to-circle-ci/</guid>
      <description>travis ci と circle ci の無償SaaS 版を比較しています。
OSS の CI では travis ci がよく使われる印象がありますが、 場合によっては circle CI に移行するとCIの時間が大幅に短くなったりして良いと思います。 ただし、複数バージョンで並列にテストしたい場合、circle ci の無償planだと並列に実行できないため、 travis でやったほうが速いかもしれません。
Circle CI の良いところ  好きな Docker Image が使える ローカルでテストが出来る Pending 時間が travis ci に比べて短い気がする(主観) private repository の CI も出来る  好きな Docker Image が使えるのが大きいですね。 予め CI に必要なツールをインストールした Image を用意しておくことで大幅に高速化出来ますし、 ツールがインストールできなかったりバージョンが変わってしまったりするトラブルも避けられます。 同じImageを使ってローカルでテストできるのでローカルでの検証もしやすいです。
自分の場合 Golang のツールの CI用に Docker Image を用意しています。
https://hub.docker.com/r/suzukishunsuke/go-ci/</description>
    </item>
    
    <item>
      <title>metabase を使って drone の利用状況を可視化する</title>
      <link>https://techblog.szksh.cloud/visualize-drone-usage-by-metabase/</link>
      <pubDate>Sun, 30 Sep 2018 22:55:57 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/visualize-drone-usage-by-metabase/</guid>
      <description>drone OSS 0.8.5 で検証しています。
https://drone.io/ と https://www.metabase.com/ の話です。
drone の利用状況を可視化したかったので、 drone の DB(MySQL) のデータを metabase で可視化しました。
どんなグラフを作ったのか いざグラフを作成するとなると、何を作ったらいいのか迷いましたが、以下のようなものをとりあえず作ってみました。
 ユーザー数 有効化されたリポジトリ数 buildの多いリポジトリ buildをよく実行しているユーザー ビルド時間の分布 build event の割合(push, tag, pull request, deployment) リポジトリのsecretsの数の分布  </description>
    </item>
    
    <item>
      <title>drone の管理のために portainer を導入した</title>
      <link>https://techblog.szksh.cloud/use-portainer-for-drone-admin/</link>
      <pubDate>Sun, 30 Sep 2018 21:56:42 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/use-portainer-for-drone-admin/</guid>
      <description>drone OSS 0.8.5 で検証しています。
https://drone.io/ と https://portainer.io/ の話です。
drone はかなり安定したシステムだと思いますが、 ユーザーが好きなイメージで好きなコマンドを実行できる以上、トラブルが起こることはあります。
その結果、 特定の build がタイムアウトにならずに延々と実行され続けるなんてことがあります。
その場合、
 どのノードのどのコンテナでトラブルが起きているのか そのコンテナで何が起こっているのか  というのを知りたいのですが、 drone にはそういった管理機能はありません。
そこで 複数のサーバで実行されている Docker コンテナを管理できるツールはないかと探したところ、 portainer が良さそうだったので導入しました。
portainer を使うと複数のサーバのコンテナを一覧で見ることが出来、 コンテナを操作(削除、停止、再起動etc)出来ます。 コンテナのログや簡単なメトリックス(CPU, memory, network usage) が見れます。 コンテナだけでなく、network や volume, image といったリソースも管理できます。
portainer の導入 公式ドキュメントに書いてありますが、 swarm cluster を構築してそこにデプロイすればよいです。
 https://docs.docker.com/engine/swarm/swarm-tutorial/ https://portainer.readthedocs.io/en/stable/deployment.html#inside-a-swarm-cluster  困っていること 毎回検索で絞り込みしないといけない 検索で絞り込んだ後に特定のコンテナの詳細画面飛んだ後一覧に戻ると 検索がクリアされているのでもう一度検索しないといけないのが面倒です。
もう少し高度な検索がしたい 自由入力の検索ボックスが1つあるだけで、特にクエリが書けるわけでもなさそうなので、 もう少し高度な検索がしたいです。 例えば Created At で 1時間以上前に特定のノードで作られたコンテナの一覧とか。
docker API でエラーが出て、コンテナを操作できない これは多分不具合とかではなく自分の設定が良くないのだと思います。 この辺の issue が関係してそうですが、まだ解消できていません。</description>
    </item>
    
    <item>
      <title>drone で非rootユーザーで実行されるImageを使えない</title>
      <link>https://techblog.szksh.cloud/drone-cant-use-nonroot-image/</link>
      <pubDate>Sun, 30 Sep 2018 21:42:16 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-cant-use-nonroot-image/</guid>
      <description>drone で非rootユーザーで実行されるDocker Imageを使おうとするとbuildに失敗します。
/bin/sh: 3: cannot create /root/.netrc: Permission denied これについては drone の開発者が回答しています。
 https://discourse.drone.io/t/solved-netrc-permission-denied/171/2 https://discourse.drone.io/t/solved-netrc-permission-denied/171/4  結論を言うと、 他のイメージを使うか、 rootで実行されるようにイメージを修正する必要がありそうです。</description>
    </item>
    
    <item>
      <title>drone の step を実行するか否かをタグ名で判定する方法</title>
      <link>https://techblog.szksh.cloud/drone-tag-conditional-step/</link>
      <pubDate>Sun, 30 Sep 2018 21:30:06 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-tag-conditional-step/</guid>
      <description>drone 0.8.5 で検証しています。
tag 名による判定方法はドキュメントには書いてないので書いておきます。
when: ref: refs/tags/foo-* # タグ名が foo-* ならステップを実行 グロブ * には / は含まれないことには注意してください。
上記の例だと、 tag foo/bar/0.1.0 はマッチしません。 これは Go の filepath.Match を使っているからです。
 https://github.com/cncd/pipeline/blob/f8c48fc9fb9fd113c6e7dd941d63bb9f86a623cb/pipeline/frontend/yaml/constraint.go#L70 https://github.com/cncd/pipeline/blob/f8c48fc9fb9fd113c6e7dd941d63bb9f86a623cb/pipeline/frontend/yaml/constraint.go#L80  </description>
    </item>
    
    <item>
      <title>drone の project visibility とは</title>
      <link>https://techblog.szksh.cloud/drone-project-visibility/</link>
      <pubDate>Sun, 30 Sep 2018 20:32:20 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-project-visibility/</guid>
      <description>恐らく公式ドキュメントに説明がない気がするので書いておきます。 と言っても、以下のissueに全部書いてありますが。
https://github.com/drone/drone/issues/2042
droneの web ui からリポジトリごとに設定できます。 そのリポジトリが誰に見えるかの設定です。
 public: ログインしていなくても誰でも見れる(publicリポジトリのデフォルト) private: リポジトリにアクセスできる人しか見れない(privateリポジトリのデフォルト) internal: ログインしていれば誰でも見れる  </description>
    </item>
    
    <item>
      <title>drone の matrix build が27個しか実行されない</title>
      <link>https://techblog.szksh.cloud/drone-matrix-build-run-only-27/</link>
      <pubDate>Sun, 30 Sep 2018 20:26:11 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-matrix-build-run-only-27/</guid>
      <description>drone 0.8.5 で検証しています。
matrix: ZOO: - 1 - 2 - 3 - 4 FOO: - 1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 BAR: - 1 - 2 上記の場合、本来 4 * 8 * 2 = 64 個実行されるはずのmatrix builds が27個しか実行されなかったのでソースコードを確認したところ、 27個しか実行されないようにハードコードされていました。
 https://github.com/cncd/pipeline/blob/d4e09fd3021a16408bc3ebdd3500efd28f51e72c/pipeline/frontend/yaml/matrix/matrix.go#L11 https://github.com/cncd/pipeline/blob/d4e09fd3021a16408bc3ebdd3500efd28f51e72c/pipeline/frontend/yaml/matrix/matrix.go#L93  関係ある部分だけ抽出すると以下のような感じになります。
limitAxis = 25 axisList := []Axis{} for p := 0; p &amp;lt; perm; p++ { axisList = append(axisList, axis) if p &amp;gt; limitAxis { break } } 制限をかけるのは仕方ないですが、 エラーも warning もなく正常終了し、でもよく見ると 27 個しか実行されていないので ユーザーとしては混乱しますね。 build の結果の画面の上の方に warning があると嬉しいです。</description>
    </item>
    
    <item>
      <title>drone の build の timeout が変更できない</title>
      <link>https://techblog.szksh.cloud/drone-build-timeout/</link>
      <pubDate>Sun, 30 Sep 2018 20:16:28 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-build-timeout/</guid>
      <description>drone 0.8.5 で検証しています。
drone の build の タイムアウトの設定はリポジトリの settings から変更できそうですが、 実は drone の admin しか変更できません。
ブラウザのデベロッパーツールを使うと、この Timeout の設定を変更した際に
PATCH /api/repos/:owner/:name にリクエストが飛んでいるので、そこからコードを追いかけると分かります。
 https://github.com/drone/drone/blob/29785b86f6534ded974120de0fcf7c21397a9d0d/router/router.go#L109 https://github.com/drone/drone/blob/29785b86f6534ded974120de0fcf7c21397a9d0d/server/repo.go#L117  </description>
    </item>
    
    <item>
      <title>Drone の build 実行時の認証方法</title>
      <link>https://techblog.szksh.cloud/drone-git-authentication/</link>
      <pubDate>Sun, 30 Sep 2018 19:44:27 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-git-authentication/</guid>
      <description>drone の build における GitHub (GitHub前提で書きますが、GitHub以外でも同じだと思います) の認証の話(どうやって認証しているか)について書いておこうと思います。 drone の build は clone step で対象のリポジトリを GitHub から clone してきています。 この際に何かしらの方法で認証しているはずです。
結論を言うと、
あるリポジトリAのbuildでは、 リポジトリAの drone連携を有効化したユーザー Bの access token を .netrc に書き込んで認証しています。 よってユーザーBにcloneする権限があるリポジトリはcloneできるし、 ユーザーBにcloneする権限がないリポジトリはcloneできません。 つまり、 誰が連携を有効化するかが重要 になります(これについては後述します)。 なお、drone連携の有効化はそのリポジトリのowner以上でないと出来ません。
drone上でリポジトリの連携を有効化すると、 リポジトリのHookが作成されます。 リポジトリの settings &amp;gt; Hooks から確認できます。 この Hook の Payload URL を見ると access_token クエリがあると思います。 JWTのようですね。これはリポジトリの連携を有効化したユーザーのtokenです。
このtokenが GitHub から drone への webhook のパラメータとして送られてくるので、 drone 側で認証し、認証したユーザーのGitHub のaccess token を取得し、 build 時にコンテナの /root/.netrc に書き込むようです。
.netrcに書き込まれているのは試しに個人の private repository で .</description>
    </item>
    
    <item>
      <title>Drone と Circle CI の workspace の扱いの違いについて</title>
      <link>https://techblog.szksh.cloud/drone-circle-volume-difference/</link>
      <pubDate>Sun, 30 Sep 2018 19:27:59 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-circle-volume-difference/</guid>
      <description>drone は同じ pipeline の step 間で同じ workspace を docker の volume としてマウントすることで workspace を共有します。
http://docs.drone.io/workspace/
circle ci はデフォルトで job 間で workspace を共有しません。 persist_to_workspace を指定することで共有する事ができます。
https://circleci.com/docs/2.0/workflows/#using-workspaces-to-share-data-among-jobs
circle ci の場合は volume を共有するのではなく、指定したディレクトリを archive し、次の job で展開することでファイルを共有するようです。
この違いには一長一短があります。
circle ci の場合は archive, unarchive する分、volume 共有に比べて時間がかかります。
そのため、下手に job を分けるより一つの job で処理したほうが処理時間が短くなる場合がありますが、 build や test といった処理は出来れば別の job として実行したいでしょうし、それでは workflow が使えません。
ただし、共有するパスは自由に選べるので必要最小限に抑えることで時間を短縮できます。
また、circle ci の場合は archive するパス及び展開先のパスを自由に選べるので自由度が高いです。 drone の場合、 workspace 以外のファイルを共有できません。
また、drone の場合 volume を共有するので同じ pipeline の step は同じノードで実行されるという制約がありますが、 circle ci の場合、別のノードでの実行が可能です。 drone の group を使って並列に実行する場合、複数のノードに分散できませんが、 circle ci の場合分散できるのでよりスケールしやすいと言えるでしょう。</description>
    </item>
    
    <item>
      <title>drone exec を並列実行した際のdocker network name の衝突について</title>
      <link>https://techblog.szksh.cloud/drone-exec-network-name-collision/</link>
      <pubDate>Sun, 30 Sep 2018 18:52:34 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/drone-exec-network-name-collision/</guid>
      <description>drone exec を並列実行すると docker network name が衝突することがあります。
$ drone exec --local &amp;amp; drone exec --local [1] 42934 2018/06/06 01:58:12 Error response from daemon: network drone_default is ambiguous (4 matches found on name) 2018/06/06 01:58:12 Error response from daemon: Conflict. The container name &amp;quot;/drone_step_0&amp;quot; is already in use by container &amp;quot;464a29b0726d6ff1a352d81df9c837330501085be550bb16abac3d338dfad887&amp;quot;. You have to remove (or rename) that container to be able to reuse that name. [1] + exit 1 drone exec --local drone は pipeline 実行時に network を作成し、pipeline が終了すると network を削除します。</description>
    </item>
    
    <item>
      <title>go-gencfg - viperの個々のアプリケーション用のラッパーのコードジェネレータ</title>
      <link>https://techblog.szksh.cloud/go-gencfg/</link>
      <pubDate>Thu, 06 Sep 2018 23:59:35 +0900</pubDate>
      
      <guid>https://techblog.szksh.cloud/go-gencfg/</guid>
      <description>自作のOSS go-gencfg を紹介します。 Golang で viper という汎用的な設定管理ライブラリがありますが、 特定のアプリケーション用に viper のラッパーを生成するCLIツールです。
使い方や開発の背景を書こうかと思いましたが、だいたい README に書いてあるので そちらを御覧ください。
https://github.com/suzuki-shunsuke/go-gencfg/blob/master/README.md</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://techblog.szksh.cloud/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://techblog.szksh.cloud/about/</guid>
      <description> https://github.com/suzuki-shunsuke/profile  </description>
    </item>
    
  </channel>
</rss>
